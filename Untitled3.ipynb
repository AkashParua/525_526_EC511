{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP67duYlPjyg3yBrIa3D/to",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "238bb4fefd5541e18a4b7bffbe0a9981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_547005a295fa4affa0573d501f5af749",
              "IPY_MODEL_0da264524b9949279f81cff7813022a6",
              "IPY_MODEL_f5d49303836c46fa9f43e4f3768bd5a9"
            ],
            "layout": "IPY_MODEL_9336f20e7c9348f9ac4b715ad203e699"
          }
        },
        "547005a295fa4affa0573d501f5af749": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cdbf5a0970245cf8518b4a57dd0ba05",
            "placeholder": "​",
            "style": "IPY_MODEL_31972053ca4c425c8d7dcf7c06dd2d18",
            "value": "100%"
          }
        },
        "0da264524b9949279f81cff7813022a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eccb4eb8546248f194dc932529f3cd4a",
            "max": 76688335,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ea0c6acb68a42378c6330ac08acf065",
            "value": 76688335
          }
        },
        "f5d49303836c46fa9f43e4f3768bd5a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be6741a4281248dfa3049714d75b9367",
            "placeholder": "​",
            "style": "IPY_MODEL_1b82937720674b34b9bb24e865cfe85b",
            "value": " 73.1M/73.1M [00:01&lt;00:00, 58.0MB/s]"
          }
        },
        "9336f20e7c9348f9ac4b715ad203e699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cdbf5a0970245cf8518b4a57dd0ba05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31972053ca4c425c8d7dcf7c06dd2d18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eccb4eb8546248f194dc932529f3cd4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ea0c6acb68a42378c6330ac08acf065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be6741a4281248dfa3049714d75b9367": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b82937720674b34b9bb24e865cfe85b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkashParua/525_526_EC511/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB5BDom96Aqd",
        "outputId": "81a05de6-2258-48b8-95e8-6861f6da6b54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  val2017.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os \n",
        "torch.cuda.get_device_name(0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "L1nszgI36crM",
        "outputId": "6254c051-1d1e-47cd-b55e-6d4889b95377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Sample.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPNP2rAu_dGf",
        "outputId": "25056281-cc54-4bcf-93ce-7360f02fc939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Sample.zip\n",
            "   creating: Sample/\n",
            "  inflating: Sample/000000000139.jpg  \n",
            "  inflating: Sample/000000000285.jpg  \n",
            "  inflating: Sample/000000000632.jpg  \n",
            "  inflating: Sample/000000000724.jpg  \n",
            "  inflating: Sample/000000000776.jpg  \n",
            "  inflating: Sample/000000000785.jpg  \n",
            "  inflating: Sample/000000000802.jpg  \n",
            "  inflating: Sample/000000000872.jpg  \n",
            "  inflating: Sample/000000000885.jpg  \n",
            "  inflating: Sample/000000001000.jpg  \n",
            "  inflating: Sample/000000001268.jpg  \n",
            "  inflating: Sample/000000001296.jpg  \n",
            "  inflating: Sample/000000001353.jpg  \n",
            "  inflating: Sample/000000001425.jpg  \n",
            "  inflating: Sample/000000001490.jpg  \n",
            "  inflating: Sample/000000001503.jpg  \n",
            "  inflating: Sample/000000001532.jpg  \n",
            "  inflating: Sample/000000001584.jpg  \n",
            "  inflating: Sample/000000001675.jpg  \n",
            "  inflating: Sample/000000001761.jpg  \n",
            "  inflating: Sample/000000001818.jpg  \n",
            "  inflating: Sample/000000001993.jpg  \n",
            "  inflating: Sample/000000002006.jpg  \n",
            "  inflating: Sample/000000002149.jpg  \n",
            "  inflating: Sample/000000002153.jpg  \n",
            "  inflating: Sample/000000002157.jpg  \n",
            "  inflating: Sample/000000002261.jpg  \n",
            "  inflating: Sample/000000002299.jpg  \n",
            "  inflating: Sample/000000002431.jpg  \n",
            "  inflating: Sample/000000002473.jpg  \n",
            "  inflating: Sample/000000002532.jpg  \n",
            "  inflating: Sample/000000002587.jpg  \n",
            "  inflating: Sample/000000002592.jpg  \n",
            "  inflating: Sample/000000002685.jpg  \n",
            "  inflating: Sample/000000002923.jpg  \n",
            "  inflating: Sample/000000003156.jpg  \n",
            "  inflating: Sample/000000003255.jpg  \n",
            "  inflating: Sample/000000003501.jpg  \n",
            "  inflating: Sample/000000003553.jpg  \n",
            "  inflating: Sample/000000003661.jpg  \n",
            "  inflating: Sample/000000003845.jpg  \n",
            "  inflating: Sample/000000003934.jpg  \n",
            "  inflating: Sample/000000004134.jpg  \n",
            "  inflating: Sample/000000004395.jpg  \n",
            "  inflating: Sample/000000004495.jpg  \n",
            "  inflating: Sample/000000004765.jpg  \n",
            "  inflating: Sample/000000004795.jpg  \n",
            "  inflating: Sample/000000005001.jpg  \n",
            "  inflating: Sample/000000005037.jpg  \n",
            "  inflating: Sample/000000005060.jpg  \n",
            "  inflating: Sample/000000005193.jpg  \n",
            "  inflating: Sample/000000005477.jpg  \n",
            "  inflating: Sample/000000005503.jpg  \n",
            "  inflating: Sample/000000005529.jpg  \n",
            "  inflating: Sample/000000005586.jpg  \n",
            "  inflating: Sample/000000005600.jpg  \n",
            "  inflating: Sample/000000005992.jpg  \n",
            "  inflating: Sample/000000006012.jpg  \n",
            "  inflating: Sample/000000006040.jpg  \n",
            "  inflating: Sample/000000006213.jpg  \n",
            "  inflating: Sample/000000006460.jpg  \n",
            "  inflating: Sample/000000006471.jpg  \n",
            "  inflating: Sample/000000006614.jpg  \n",
            "  inflating: Sample/000000006723.jpg  \n",
            "  inflating: Sample/000000006763.jpg  \n",
            "  inflating: Sample/000000006771.jpg  \n",
            "  inflating: Sample/000000006818.jpg  \n",
            "  inflating: Sample/000000006894.jpg  \n",
            "  inflating: Sample/000000006954.jpg  \n",
            "  inflating: Sample/000000007088.jpg  \n",
            "  inflating: Sample/000000007108.jpg  \n",
            "  inflating: Sample/000000007278.jpg  \n",
            "  inflating: Sample/000000007281.jpg  \n",
            "  inflating: Sample/000000007386.jpg  \n",
            "  inflating: Sample/000000007511.jpg  \n",
            "  inflating: Sample/000000007574.jpg  \n",
            "  inflating: Sample/000000007784.jpg  \n",
            "  inflating: Sample/000000007795.jpg  \n",
            "  inflating: Sample/000000007816.jpg  \n",
            "  inflating: Sample/000000007818.jpg  \n",
            "  inflating: Sample/000000007888.jpg  \n",
            "  inflating: Sample/000000007977.jpg  \n",
            "  inflating: Sample/000000007991.jpg  \n",
            "  inflating: Sample/000000008021.jpg  \n",
            "  inflating: Sample/000000008211.jpg  \n",
            "  inflating: Sample/000000008277.jpg  \n",
            "  inflating: Sample/000000008532.jpg  \n",
            "  inflating: Sample/000000008629.jpg  \n",
            "  inflating: Sample/000000008690.jpg  \n",
            "  inflating: Sample/000000008762.jpg  \n",
            "  inflating: Sample/000000008844.jpg  \n",
            "  inflating: Sample/000000008899.jpg  \n",
            "  inflating: Sample/000000009378.jpg  \n",
            "  inflating: Sample/000000009400.jpg  \n",
            "  inflating: Sample/000000009448.jpg  \n",
            "  inflating: Sample/000000009483.jpg  \n",
            "  inflating: Sample/000000009590.jpg  \n",
            "  inflating: Sample/000000009769.jpg  \n",
            "  inflating: Sample/000000009772.jpg  \n",
            "  inflating: Sample/000000009891.jpg  \n",
            "  inflating: Sample/000000009914.jpg  \n",
            "  inflating: Sample/000000010092.jpg  \n",
            "  inflating: Sample/000000010363.jpg  \n",
            "  inflating: Sample/000000010583.jpg  \n",
            "  inflating: Sample/000000010707.jpg  \n",
            "  inflating: Sample/000000010764.jpg  \n",
            "  inflating: Sample/000000010977.jpg  \n",
            "  inflating: Sample/000000010995.jpg  \n",
            "  inflating: Sample/000000011051.jpg  \n",
            "  inflating: Sample/000000011122.jpg  \n",
            "  inflating: Sample/000000011149.jpg  \n",
            "  inflating: Sample/000000011197.jpg  \n",
            "  inflating: Sample/000000011511.jpg  \n",
            "  inflating: Sample/000000011615.jpg  \n",
            "  inflating: Sample/000000011699.jpg  \n",
            "  inflating: Sample/000000011760.jpg  \n",
            "  inflating: Sample/000000011813.jpg  \n",
            "  inflating: Sample/000000012062.jpg  \n",
            "  inflating: Sample/000000012120.jpg  \n",
            "  inflating: Sample/000000012280.jpg  \n",
            "  inflating: Sample/000000012576.jpg  \n",
            "  inflating: Sample/000000012639.jpg  \n",
            "  inflating: Sample/000000012667.jpg  \n",
            "  inflating: Sample/000000012670.jpg  \n",
            "  inflating: Sample/000000012748.jpg  \n",
            "  inflating: Sample/000000013004.jpg  \n",
            "  inflating: Sample/000000013177.jpg  \n",
            "  inflating: Sample/000000013201.jpg  \n",
            "  inflating: Sample/000000013291.jpg  \n",
            "  inflating: Sample/000000013348.jpg  \n",
            "  inflating: Sample/000000013546.jpg  \n",
            "  inflating: Sample/000000013597.jpg  \n",
            "  inflating: Sample/000000013659.jpg  \n",
            "  inflating: Sample/000000013729.jpg  \n",
            "  inflating: Sample/000000013774.jpg  \n",
            "  inflating: Sample/000000013923.jpg  \n",
            "  inflating: Sample/000000014007.jpg  \n",
            "  inflating: Sample/000000014038.jpg  \n",
            "  inflating: Sample/000000014226.jpg  \n",
            "  inflating: Sample/000000014380.jpg  \n",
            "  inflating: Sample/000000014439.jpg  \n",
            "  inflating: Sample/000000014473.jpg  \n",
            "  inflating: Sample/000000014831.jpg  \n",
            "  inflating: Sample/000000014888.jpg  \n",
            "  inflating: Sample/000000015079.jpg  \n",
            "  inflating: Sample/000000015254.jpg  \n",
            "  inflating: Sample/000000015272.jpg  \n",
            "  inflating: Sample/000000015278.jpg  \n",
            "  inflating: Sample/000000015335.jpg  \n",
            "  inflating: Sample/000000015338.jpg  \n",
            "  inflating: Sample/000000015440.jpg  \n",
            "  inflating: Sample/000000015497.jpg  \n",
            "  inflating: Sample/000000015517.jpg  \n",
            "  inflating: Sample/000000015597.jpg  \n",
            "  inflating: Sample/000000015660.jpg  \n",
            "  inflating: Sample/000000015746.jpg  \n",
            "  inflating: Sample/000000015751.jpg  \n",
            "  inflating: Sample/000000015956.jpg  \n",
            "  inflating: Sample/000000016010.jpg  \n",
            "  inflating: Sample/000000016228.jpg  \n",
            "  inflating: Sample/000000016249.jpg  \n",
            "  inflating: Sample/000000016439.jpg  \n",
            "  inflating: Sample/000000016451.jpg  \n",
            "  inflating: Sample/000000016502.jpg  \n",
            "  inflating: Sample/000000016598.jpg  \n",
            "  inflating: Sample/000000016958.jpg  \n",
            "  inflating: Sample/000000017029.jpg  \n",
            "  inflating: Sample/000000017031.jpg  \n",
            "  inflating: Sample/000000017115.jpg  \n",
            "  inflating: Sample/000000017178.jpg  \n",
            "  inflating: Sample/000000017182.jpg  \n",
            "  inflating: Sample/000000017207.jpg  \n",
            "  inflating: Sample/000000017379.jpg  \n",
            "  inflating: Sample/000000017436.jpg  \n",
            "  inflating: Sample/000000017627.jpg  \n",
            "  inflating: Sample/000000017714.jpg  \n",
            "  inflating: Sample/000000017899.jpg  \n",
            "  inflating: Sample/000000017905.jpg  \n",
            "  inflating: Sample/000000017959.jpg  \n",
            "  inflating: Sample/000000018150.jpg  \n",
            "  inflating: Sample/000000018193.jpg  \n",
            "  inflating: Sample/000000018380.jpg  \n",
            "  inflating: Sample/000000018491.jpg  \n",
            "  inflating: Sample/000000018519.jpg  \n",
            "  inflating: Sample/000000018575.jpg  \n",
            "  inflating: Sample/000000018737.jpg  \n",
            "  inflating: Sample/000000018770.jpg  \n",
            "  inflating: Sample/000000018833.jpg  \n",
            "  inflating: Sample/000000018837.jpg  \n",
            "  inflating: Sample/000000019042.jpg  \n",
            "  inflating: Sample/000000019109.jpg  \n",
            "  inflating: Sample/000000019221.jpg  \n",
            "  inflating: Sample/000000019402.jpg  \n",
            "  inflating: Sample/000000019432.jpg  \n",
            "  inflating: Sample/000000019742.jpg  \n",
            "  inflating: Sample/000000019786.jpg  \n",
            "  inflating: Sample/000000019924.jpg  \n",
            "  inflating: Sample/000000020059.jpg  \n",
            "  inflating: Sample/000000020107.jpg  \n",
            "  inflating: Sample/000000020247.jpg  \n",
            "  inflating: Sample/000000020333.jpg  \n",
            "  inflating: Sample/000000020553.jpg  \n",
            "  inflating: Sample/000000020571.jpg  \n",
            "  inflating: Sample/000000020992.jpg  \n",
            "  inflating: Sample/000000021167.jpg  \n",
            "  inflating: Sample/000000021465.jpg  \n",
            "  inflating: Sample/000000021503.jpg  \n",
            "  inflating: Sample/000000021604.jpg  \n",
            "  inflating: Sample/000000021839.jpg  \n",
            "  inflating: Sample/000000021879.jpg  \n",
            "  inflating: Sample/000000021903.jpg  \n",
            "  inflating: Sample/000000022192.jpg  \n",
            "  inflating: Sample/000000022371.jpg  \n",
            "  inflating: Sample/000000022396.jpg  \n",
            "  inflating: Sample/000000022479.jpg  \n",
            "  inflating: Sample/000000022589.jpg  \n",
            "  inflating: Sample/000000022623.jpg  \n",
            "  inflating: Sample/000000022705.jpg  \n",
            "  inflating: Sample/000000022755.jpg  \n",
            "  inflating: Sample/000000022892.jpg  \n",
            "  inflating: Sample/000000022935.jpg  \n",
            "  inflating: Sample/000000022969.jpg  \n",
            "  inflating: Sample/000000023023.jpg  \n",
            "  inflating: Sample/000000023034.jpg  \n",
            "  inflating: Sample/000000023126.jpg  \n",
            "  inflating: Sample/000000023230.jpg  \n",
            "  inflating: Sample/000000023272.jpg  \n",
            "  inflating: Sample/000000023359.jpg  \n",
            "  inflating: Sample/000000023666.jpg  \n",
            "  inflating: Sample/000000023751.jpg  \n",
            "  inflating: Sample/000000023781.jpg  \n",
            "  inflating: Sample/000000023899.jpg  \n",
            "  inflating: Sample/000000023937.jpg  \n",
            "  inflating: Sample/000000024021.jpg  \n",
            "  inflating: Sample/000000024027.jpg  \n",
            "  inflating: Sample/000000024144.jpg  \n",
            "  inflating: Sample/000000024243.jpg  \n",
            "  inflating: Sample/000000024567.jpg  \n",
            "  inflating: Sample/000000024610.jpg  \n",
            "  inflating: Sample/000000024919.jpg  \n",
            "  inflating: Sample/000000025057.jpg  \n",
            "  inflating: Sample/000000025096.jpg  \n",
            "  inflating: Sample/000000025139.jpg  \n",
            "  inflating: Sample/000000025181.jpg  \n",
            "  inflating: Sample/000000025228.jpg  \n",
            "  inflating: Sample/000000025386.jpg  \n",
            "  inflating: Sample/000000025393.jpg  \n",
            "  inflating: Sample/000000025394.jpg  \n",
            "  inflating: Sample/000000025424.jpg  \n",
            "  inflating: Sample/000000025560.jpg  \n",
            "  inflating: Sample/000000025593.jpg  \n",
            "  inflating: Sample/000000025603.jpg  \n",
            "  inflating: Sample/000000025986.jpg  \n",
            "  inflating: Sample/000000026204.jpg  \n",
            "  inflating: Sample/000000026465.jpg  \n",
            "  inflating: Sample/000000026564.jpg  \n",
            "  inflating: Sample/000000026690.jpg  \n",
            "  inflating: Sample/000000026926.jpg  \n",
            "  inflating: Sample/000000026941.jpg  \n",
            "  inflating: Sample/000000027186.jpg  \n",
            "  inflating: Sample/000000027620.jpg  \n",
            "  inflating: Sample/000000027696.jpg  \n",
            "  inflating: Sample/000000027768.jpg  \n",
            "  inflating: Sample/000000027932.jpg  \n",
            "  inflating: Sample/000000027972.jpg  \n",
            "  inflating: Sample/000000027982.jpg  \n",
            "  inflating: Sample/000000028285.jpg  \n",
            "  inflating: Sample/000000028449.jpg  \n",
            "  inflating: Sample/000000028452.jpg  \n",
            "  inflating: Sample/000000028809.jpg  \n",
            "  inflating: Sample/000000028993.jpg  \n",
            "  inflating: Sample/000000029187.jpg  \n",
            "  inflating: Sample/000000029393.jpg  \n",
            "  inflating: Sample/000000029397.jpg  \n",
            "  inflating: Sample/000000029596.jpg  \n",
            "  inflating: Sample/000000029640.jpg  \n",
            "  inflating: Sample/000000029675.jpg  \n",
            "  inflating: Sample/000000029984.jpg  \n",
            "  inflating: Sample/000000030213.jpg  \n",
            "  inflating: Sample/000000030494.jpg  \n",
            "  inflating: Sample/000000030504.jpg  \n",
            "  inflating: Sample/000000030675.jpg  \n",
            "  inflating: Sample/000000030785.jpg  \n",
            "  inflating: Sample/000000030828.jpg  \n",
            "  inflating: Sample/000000031050.jpg  \n",
            "  inflating: Sample/000000031093.jpg  \n",
            "  inflating: Sample/000000031118.jpg  \n",
            "  inflating: Sample/000000031217.jpg  \n",
            "  inflating: Sample/000000031248.jpg  \n",
            "  inflating: Sample/000000031269.jpg  \n",
            "  inflating: Sample/000000031296.jpg  \n",
            "  inflating: Sample/000000031322.jpg  \n",
            "  inflating: Sample/000000031620.jpg  \n",
            "  inflating: Sample/000000031735.jpg  \n",
            "  inflating: Sample/000000031749.jpg  \n",
            "  inflating: Sample/000000031817.jpg  \n",
            "  inflating: Sample/000000032038.jpg  \n",
            "  inflating: Sample/000000032081.jpg  \n",
            "  inflating: Sample/000000032285.jpg  \n",
            "  inflating: Sample/000000032334.jpg  \n",
            "  inflating: Sample/000000032570.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install memory_profiler super_gradients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xmykV82wAtGx",
        "outputId": "474d8ee1-617b-424c-99f2-c8fba60130db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting memory_profiler\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Collecting super_gradients\n",
            "  Downloading super_gradients-3.1.1-py3-none-any.whl (964 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m964.2/964.2 kB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory_profiler) (5.9.5)\n",
            "Collecting torch<1.14,>=1.9.0 (from super_gradients)\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from super_gradients) (4.65.0)\n",
            "Collecting boto3>=1.17.15 (from super_gradients)\n",
            "  Downloading boto3-1.26.147-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from super_gradients) (4.3.3)\n",
            "Collecting Deprecated>=1.2.11 (from super_gradients)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: opencv-python>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from super_gradients) (4.7.0.72)\n",
            "Requirement already satisfied: scipy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from super_gradients) (1.10.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.10/dist-packages (from super_gradients) (3.7.1)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from super_gradients) (2.12.2)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from super_gradients) (67.7.2)\n",
            "Collecting coverage~=5.3.1 (from super_gradients)\n",
            "  Downloading coverage-5.3.1.tar.gz (684 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m684.5/684.5 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from super_gradients) (0.15.2+cu118)\n",
            "Collecting sphinx~=4.0.2 (from super_gradients)\n",
            "  Downloading Sphinx-4.0.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinx-rtd-theme (from super_gradients)\n",
            "  Downloading sphinx_rtd_theme-1.2.1-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics==0.8 (from super_gradients)\n",
            "  Downloading torchmetrics-0.8.0-py3-none-any.whl (408 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.6/408.6 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow>=9.2.0 (from super_gradients)\n",
            "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.2.0 (from super_gradients)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf (from super_gradients)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime==1.13.1 (from super_gradients)\n",
            "  Downloading onnxruntime-1.13.1-cp310-cp310-manylinux_2_27_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx==1.13.0 (from super_gradients)\n",
            "  Downloading onnx-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pip-tools>=6.12.1 in /usr/local/lib/python3.10/dist-packages (from super_gradients) (6.13.0)\n",
            "Collecting pyparsing==2.4.5 (from super_gradients)\n",
            "  Downloading pyparsing-2.4.5-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops==0.3.2 (from super_gradients)\n",
            "  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Collecting pycocotools==2.0.4 (from super_gradients)\n",
            "  Downloading pycocotools-2.0.4.tar.gz (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.10/dist-packages (from super_gradients) (3.20.3)\n",
            "Collecting treelib==1.6.1 (from super_gradients)\n",
            "  Downloading treelib-1.6.1.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting termcolor==1.1.0 (from super_gradients)\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging>=20.4 in /usr/local/lib/python3.10/dist-packages (from super_gradients) (23.1)\n",
            "Requirement already satisfied: wheel>=0.38.0 in /usr/local/lib/python3.10/dist-packages (from super_gradients) (0.40.0)\n",
            "Requirement already satisfied: pygments>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from super_gradients) (2.14.0)\n",
            "Collecting stringcase>=1.2.0 (from super_gradients)\n",
            "  Downloading stringcase-1.2.0.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<=1.23 in /usr/local/lib/python3.10/dist-packages (from super_gradients) (1.22.4)\n",
            "Collecting rapidfuzz (from super_gradients)\n",
            "  Downloading rapidfuzz-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting json-tricks==3.16.1 (from super_gradients)\n",
            "  Downloading json_tricks-3.16.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting onnx-simplifier<1.0,>=0.3.6 (from super_gradients)\n",
            "  Downloading onnx_simplifier-0.4.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx==1.13.0->super_gradients) (4.5.0)\n",
            "Collecting coloredlogs (from onnxruntime==1.13.1->super_gradients)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m503.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.13.1->super_gradients) (23.3.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.13.1->super_gradients) (1.11.1)\n",
            "Collecting pyDeprecate==0.3.* (from torchmetrics==0.8->super_gradients)\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from treelib==1.6.1->super_gradients) (0.18.3)\n",
            "Collecting botocore<1.30.0,>=1.29.147 (from boto3>=1.17.15->super_gradients)\n",
            "  Downloading botocore-1.29.147-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.17.15->super_gradients)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3>=1.17.15->super_gradients)\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated>=1.2.11->super_gradients) (1.14.1)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.2.0->super_gradients)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->super_gradients) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->super_gradients) (0.19.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->super_gradients) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->super_gradients) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->super_gradients) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->super_gradients) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->super_gradients) (2.8.2)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf->super_gradients) (6.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from onnx-simplifier<1.0,>=0.3.6->super_gradients) (13.3.4)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.10/dist-packages (from pip-tools>=6.12.1->super_gradients) (0.10.0)\n",
            "Requirement already satisfied: click>=8 in /usr/local/lib/python3.10/dist-packages (from pip-tools>=6.12.1->super_gradients) (8.1.3)\n",
            "Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.10/dist-packages (from pip-tools>=6.12.1->super_gradients) (23.1.2)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super_gradients) (1.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super_gradients) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super_gradients) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super_gradients) (2.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super_gradients) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super_gradients) (1.0.3)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super_gradients) (3.1.2)\n",
            "Requirement already satisfied: docutils<0.18,>=0.14 in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super_gradients) (0.16)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super_gradients) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super_gradients) (2.12.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super_gradients) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super_gradients) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super_gradients) (2.27.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super_gradients) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super_gradients) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super_gradients) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super_gradients) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super_gradients) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super_gradients) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super_gradients) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super_gradients) (2.3.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<1.14,>=1.9.0->super_gradients)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<1.14,>=1.9.0->super_gradients)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<1.14,>=1.9.0->super_gradients)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<1.14,>=1.9.0->super_gradients)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision>=0.10.0 (from super_gradients)\n",
            "  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m118.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinxcontrib-jquery!=3.0.0,>=2.0.0 (from sphinx-rtd-theme->super_gradients)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.147->boto3>=1.17.15->super_gradients) (1.26.15)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->super_gradients) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->super_gradients) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->super_gradients) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->super_gradients) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->super_gradients) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx~=4.0.2->super_gradients) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx~=4.0.2->super_gradients) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx~=4.0.2->super_gradients) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx~=4.0.2->super_gradients) (3.4)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build->pip-tools>=6.12.1->super_gradients) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build->pip-tools>=6.12.1->super_gradients) (2.0.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime==1.13.1->super_gradients)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnx-simplifier<1.0,>=0.3.6->super_gradients) (2.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime==1.13.1->super_gradients) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->onnx-simplifier<1.0,>=0.3.6->super_gradients) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->super_gradients) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->super_gradients) (3.2.2)\n",
            "Building wheels for collected packages: pycocotools, termcolor, treelib, coverage, antlr4-python3-runtime, stringcase\n",
            "  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0.4-cp310-cp310-linux_x86_64.whl size=398145 sha256=18bd5aca42ff1addfdbc4c1cb433a45095ab1471f67798c7437b25fb297ed5f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/32/80/d5fe20c79fd05d626c0bc0ee44294db21b674267b2271588fa\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=e30421cbda05507a624c8f9fc85edea606205fd84fc0e2cc459710b36e590f63\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n",
            "  Building wheel for treelib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for treelib: filename=treelib-1.6.1-py3-none-any.whl size=18368 sha256=a692fe127d3c972c977e621e26faf630d7b8dfd6e8b271da581bbd5dc43b6d19\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/72/8b/76569b82bf280a03c4e294c3b29ee2398217186369c427ed4b\n",
            "  Building wheel for coverage (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for coverage: filename=coverage-5.3.1-cp310-cp310-linux_x86_64.whl size=243137 sha256=46d43cb4ed0c3f66b942424f27db545cdfaa6f0e2e4dbae35b52e3446d661be0\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/70/10/313be697f460d6024cfa94b7f0e22ffc1c53aab718fb4f42af\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=4a9c33627b37ff0620a5a11105fcf8618203709e29478d502c72f1d17eea959d\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for stringcase (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stringcase: filename=stringcase-1.2.0-py3-none-any.whl size=3569 sha256=6fc30f137cc79a67078899c130d04d58f65e054fc4742d3d50bfa88ef1b1a9f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/ba/22/1a2d952a9ce8aa86e42fda41e2c87fdaf20e238c88bf8df013\n",
            "Successfully built pycocotools termcolor treelib coverage antlr4-python3-runtime stringcase\n",
            "Installing collected packages: termcolor, stringcase, json-tricks, einops, antlr4-python3-runtime, treelib, rapidfuzz, pyparsing, pyDeprecate, pillow, onnx, omegaconf, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, memory_profiler, jmespath, humanfriendly, Deprecated, coverage, sphinx, nvidia-cudnn-cu11, hydra-core, coloredlogs, botocore, torch, sphinxcontrib-jquery, s3transfer, pycocotools, onnxruntime, onnx-simplifier, torchvision, torchmetrics, sphinx-rtd-theme, boto3, super_gradients\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.3.0\n",
            "    Uninstalling termcolor-2.3.0:\n",
            "      Successfully uninstalled termcolor-2.3.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 3.5.4\n",
            "    Uninstalling Sphinx-3.5.4:\n",
            "      Successfully uninstalled Sphinx-3.5.4\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.6\n",
            "    Uninstalling pycocotools-2.0.6:\n",
            "      Successfully uninstalled pycocotools-2.0.6\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2+cu118\n",
            "    Uninstalling torchvision-0.15.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.2+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Deprecated-1.2.14 antlr4-python3-runtime-4.9.3 boto3-1.26.147 botocore-1.29.147 coloredlogs-15.0.1 coverage-5.3.1 einops-0.3.2 humanfriendly-10.0 hydra-core-1.3.2 jmespath-1.0.1 json-tricks-3.16.1 memory_profiler-0.61.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 omegaconf-2.3.0 onnx-1.13.0 onnx-simplifier-0.4.31 onnxruntime-1.13.1 pillow-9.5.0 pyDeprecate-0.3.2 pycocotools-2.0.4 pyparsing-2.4.5 rapidfuzz-3.1.0 s3transfer-0.6.1 sphinx-4.0.3 sphinx-rtd-theme-1.2.1 sphinxcontrib-jquery-4.1 stringcase-1.2.0 super_gradients-3.1.1 termcolor-1.1.0 torch-1.13.1 torchmetrics-0.8.0 torchvision-0.14.1 treelib-1.6.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "pydevd_plugins",
                  "pyparsing",
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install super-gradients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjVOTj5gBJMB",
        "outputId": "9bd2587a-8b8e-4128-9a53-93db07df48e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: super-gradients in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: torch<1.14,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (4.65.0)\n",
            "Requirement already satisfied: boto3>=1.17.15 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (1.26.147)\n",
            "Requirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (4.3.3)\n",
            "Requirement already satisfied: Deprecated>=1.2.11 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (1.2.14)\n",
            "Requirement already satisfied: opencv-python>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (4.7.0.72)\n",
            "Requirement already satisfied: scipy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (1.10.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (3.7.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (5.9.5)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (2.12.2)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (67.7.2)\n",
            "Requirement already satisfied: coverage~=5.3.1 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (5.3.1)\n",
            "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (0.14.1)\n",
            "Requirement already satisfied: sphinx~=4.0.2 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (4.0.3)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.10/dist-packages (from super-gradients) (1.2.1)\n",
            "Requirement already satisfied: torchmetrics==0.8 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (0.8.0)\n",
            "Requirement already satisfied: pillow>=9.2.0 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (9.5.0)\n",
            "Requirement already satisfied: hydra-core>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (1.3.2)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (from super-gradients) (2.3.0)\n",
            "Requirement already satisfied: onnxruntime==1.13.1 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (1.13.1)\n",
            "Requirement already satisfied: onnx==1.13.0 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (1.13.0)\n",
            "Requirement already satisfied: pip-tools>=6.12.1 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (6.13.0)\n",
            "Requirement already satisfied: pyparsing==2.4.5 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (2.4.5)\n",
            "Requirement already satisfied: einops==0.3.2 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (0.3.2)\n",
            "Requirement already satisfied: pycocotools==2.0.4 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (2.0.4)\n",
            "Requirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (3.20.3)\n",
            "Requirement already satisfied: treelib==1.6.1 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (1.6.1)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (1.1.0)\n",
            "Requirement already satisfied: packaging>=20.4 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (23.1)\n",
            "Requirement already satisfied: wheel>=0.38.0 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (0.40.0)\n",
            "Requirement already satisfied: pygments>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (2.14.0)\n",
            "Requirement already satisfied: stringcase>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (1.2.0)\n",
            "Requirement already satisfied: numpy<=1.23 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (1.22.4)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from super-gradients) (3.1.0)\n",
            "Requirement already satisfied: json-tricks==3.16.1 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (3.16.1)\n",
            "Requirement already satisfied: onnx-simplifier<1.0,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from super-gradients) (0.4.31)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx==1.13.0->super-gradients) (4.5.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.13.1->super-gradients) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.13.1->super-gradients) (23.3.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.13.1->super-gradients) (1.11.1)\n",
            "Requirement already satisfied: pyDeprecate==0.3.* in /usr/local/lib/python3.10/dist-packages (from torchmetrics==0.8->super-gradients) (0.3.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from treelib==1.6.1->super-gradients) (0.18.3)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.147 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.17.15->super-gradients) (1.29.147)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.17.15->super-gradients) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.17.15->super-gradients) (0.6.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated>=1.2.11->super-gradients) (1.14.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.2.0->super-gradients) (4.9.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->super-gradients) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->super-gradients) (0.19.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->super-gradients) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->super-gradients) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->super-gradients) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->super-gradients) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->super-gradients) (2.8.2)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf->super-gradients) (6.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from onnx-simplifier<1.0,>=0.3.6->super-gradients) (13.3.4)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.10/dist-packages (from pip-tools>=6.12.1->super-gradients) (0.10.0)\n",
            "Requirement already satisfied: click>=8 in /usr/local/lib/python3.10/dist-packages (from pip-tools>=6.12.1->super-gradients) (8.1.3)\n",
            "Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.10/dist-packages (from pip-tools>=6.12.1->super-gradients) (23.1.2)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients) (1.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients) (2.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients) (1.0.3)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients) (3.1.2)\n",
            "Requirement already satisfied: docutils<0.18,>=0.14 in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients) (0.16)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients) (2.12.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients) (2.27.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super-gradients) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super-gradients) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super-gradients) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super-gradients) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super-gradients) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super-gradients) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super-gradients) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super-gradients) (2.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch<1.14,>=1.9.0->super-gradients) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch<1.14,>=1.9.0->super-gradients) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch<1.14,>=1.9.0->super-gradients) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch<1.14,>=1.9.0->super-gradients) (11.7.99)\n",
            "Requirement already satisfied: sphinxcontrib-jquery!=3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->super-gradients) (4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.147->boto3>=1.17.15->super-gradients) (1.26.15)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->super-gradients) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->super-gradients) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->super-gradients) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->super-gradients) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->super-gradients) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx~=4.0.2->super-gradients) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx~=4.0.2->super-gradients) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx~=4.0.2->super-gradients) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx~=4.0.2->super-gradients) (3.4)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build->pip-tools>=6.12.1->super-gradients) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build->pip-tools>=6.12.1->super-gradients) (2.0.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime==1.13.1->super-gradients) (10.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnx-simplifier<1.0,>=0.3.6->super-gradients) (2.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime==1.13.1->super-gradients) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->onnx-simplifier<1.0,>=0.3.6->super-gradients) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->super-gradients) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->super-gradients) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9gjmgguCkAV",
        "outputId": "6616b612-10bb-44a7-b693-50f7ef0c366c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "from super_gradients.training import models\n",
        "import time\n",
        "from memory_profiler import profile\n",
        "import numpy as np\n",
        "yolo_nas_l = models.get(\"yolo_nas_s\", pretrained_weights=\"coco\")\n"
      ],
      "metadata": {
        "id": "s1qAJGs0AXIM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "238bb4fefd5541e18a4b7bffbe0a9981",
            "547005a295fa4affa0573d501f5af749",
            "0da264524b9949279f81cff7813022a6",
            "f5d49303836c46fa9f43e4f3768bd5a9",
            "9336f20e7c9348f9ac4b715ad203e699",
            "5cdbf5a0970245cf8518b4a57dd0ba05",
            "31972053ca4c425c8d7dcf7c06dd2d18",
            "eccb4eb8546248f194dc932529f3cd4a",
            "5ea0c6acb68a42378c6330ac08acf065",
            "be6741a4281248dfa3049714d75b9367",
            "1b82937720674b34b9bb24e865cfe85b"
          ]
        },
        "outputId": "cb19fd27-6369-4b9d-afa5-b2bff85a6403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The console stream is logged into /root/sg_logs/console.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2023-06-06 05:48:31] INFO - crash_tips_setup.py - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it\n",
            "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
            "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
            "[2023-06-06 05:48:36] WARNING - __init__.py - Failed to import pytorch_quantization\n",
            "[2023-06-06 05:48:36] WARNING - calibrator.py - Failed to import pytorch_quantization\n",
            "[2023-06-06 05:48:36] WARNING - export.py - Failed to import pytorch_quantization\n",
            "[2023-06-06 05:48:36] WARNING - selective_quantization_utils.py - Failed to import pytorch_quantization\n",
            "[2023-06-06 05:48:37] INFO - checkpoint_utils.py - License Notification: YOLO-NAS pre-trained weights are subjected to the specific license terms and conditions detailed in \n",
            "https://github.com/Deci-AI/super-gradients/blob/master/LICENSE.YOLONAS.md\n",
            "By downloading the pre-trained weight files you agree to comply with these terms.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/73.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "238bb4fefd5541e18a4b7bffbe0a9981"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://sghub.deci.ai/models/yolo_nas_s_coco.pth\" to /root/.cache/torch/hub/checkpoints/yolo_nas_s_coco.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJF9uaFj17aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67f0876c-de61-42fb-a15d-52cd89839d85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jun  6 04:56:04 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = yolo_nas_l.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "sYSr9BJ9FJnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yolonas(src, t):       \n",
        "    image = Image.open(src)\n",
        "    resized_image = image.resize((640,640))\n",
        "    start = time.time()\n",
        "    out = model.predict(resized_image)\n",
        "    tt = t + (time.time() - start)\n",
        "    \n",
        "    width, height = image.size\n",
        "    resized_width, resized_height = resized_image.size\n",
        "    scale_x = width / resized_width\n",
        "    scale_y = height / resized_height\n",
        "\n",
        "    prediction_objects = list(out._images_prediction_lst)[0]\n",
        "    bbox = prediction_objects.prediction.bboxes_xyxy\n",
        "\n",
        "    int_labels = prediction_objects.prediction.labels.astype(int)\n",
        "    class_names = prediction_objects.class_names\n",
        "    nm = [class_names[i] for i in int_labels]\n",
        "\n",
        "    conf = prediction_objects.prediction.confidence.astype(float)\n",
        "   \n",
        "    fname = f'yolo_nas_coco/'+src[13:-4]+'.txt'\n",
        "    open(fname, 'w').close()\n",
        "    with open(fname, 'w') as f:\n",
        "        for i in range(len(bbox)):\n",
        "            s = str(nm[i])\n",
        "            #f.write(s.replace(' ','_')+\" \")\n",
        "            if s in {'cat', 'dog', 'cow', 'elephant', 'horse', 'bear', 'sheep', 'bird'}:\n",
        "                f.write(\"animal \")\n",
        "            elif s in {\"laptop\",\"mouse\",\"remote\",\"keyboard\",\"cell phone\",\"microwave\",\"oven\",\"toaster\",\"sink\",\n",
        "\t                \"refrigerator\",\"book\",\"clock\",\"vase\",\"scissors\",\"teddy bear\",\"hair drier\",\"toothbrush\",\n",
        "                    \"backpack\",\"umbrella\",\"handbag\",\"tie\",\"suitcase\",\"frisbee\",\"skis\",\n",
        "\t                \"snowboard\",\"sports ball\",\"kite\",\"baseball bat\",\"baseball glove\",\"skateboard\",\n",
        "\t                \"surfboard\",\"tennis racket\",\"bottle\",\"wine glass\",\"cup\",\"fork\",\"knife\",\"spoon\",\n",
        "\t                \"bowl\",\"banana\",\"apple\",\"sandwich\",\"orange\",\"broccoli\",\"carrot\",\"hot dog\",\"pizza\",\n",
        "\t                \"donut\",\"cake\",\"chair\",\"couch\",\"potted plant\",\"bed\",\"dining table\",\"toilet\",\"tv\"}:\n",
        "                f.write(\"other \")\n",
        "            else :\n",
        "                f.write(s.replace(' ','_')+\" \")\n",
        "            f.write(str((int(conf[i]*100))/100)+\" \")\n",
        "            f.write(str((int(bbox[i][0]*100))/100 * scale_x ) +\" \")\n",
        "            f.write(str((int(bbox[i][1]*100))/100 * scale_y )+\" \")\n",
        "            f.write(str((int((bbox[i][2]-bbox[i][0])*100))/100 * scale_x)+\" \")\n",
        "            f.write(str((int((bbox[i][3]-bbox[i][1])*100))/100 * scale_y))\n",
        "            if i!=(len(bbox)-1):\n",
        "                f.write('\\n')\n",
        "    \n",
        "    return tt"
      ],
      "metadata": {
        "id": "tifgQc-cE6ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@profile\n",
        "def func():\n",
        "  t = 0\n",
        "  l = len(os.listdir('Sample'))\n",
        "  for img in os.listdir('Sample'):\n",
        "    file_path = os.path.join('Sample',img)\n",
        "    print(file_path)\n",
        "    t = yolonas(file_path , t)\n",
        "  print(t)\n",
        "  print(torch.cuda.is_available())\n",
        "  print(f'avg time is {t/l}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IhtlcikmFQ0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlZdsPpWP0rM",
        "outputId": "b40e9603-a541-41ec-dbef-72ecffffd4dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: Could not find file <ipython-input-27-bba3fdbbfab2>\n",
            "NOTE: %mprun can only be used on functions defined in physical files, and not in the IPython environment.\n",
            "Sample/000000003845.jpg\n",
            "Sample/000000029984.jpg\n",
            "Sample/000000011699.jpg\n",
            "Sample/000000029596.jpg\n",
            "Sample/000000007088.jpg\n",
            "Sample/000000023034.jpg\n",
            "Sample/000000003553.jpg\n",
            "Sample/000000012062.jpg\n",
            "Sample/000000013774.jpg\n",
            "Sample/000000007511.jpg\n",
            "Sample/000000016451.jpg\n",
            "Sample/000000018737.jpg\n",
            "Sample/000000017905.jpg\n",
            "Sample/000000027768.jpg\n",
            "Sample/000000001490.jpg\n",
            "Sample/000000010995.jpg\n",
            "Sample/000000025096.jpg\n",
            "Sample/000000019924.jpg\n",
            "Sample/000000016958.jpg\n",
            "Sample/000000001503.jpg\n",
            "Sample/000000015746.jpg\n",
            "Sample/000000006213.jpg\n",
            "Sample/000000017207.jpg\n",
            "Sample/000000018519.jpg\n",
            "Sample/000000006614.jpg\n",
            "Sample/000000006471.jpg\n",
            "Sample/000000023359.jpg\n",
            "Sample/000000017959.jpg\n",
            "Sample/000000003501.jpg\n",
            "Sample/000000028452.jpg\n",
            "Sample/000000027696.jpg\n",
            "Sample/000000015278.jpg\n",
            "Sample/000000013546.jpg\n",
            "Sample/000000020553.jpg\n",
            "Sample/000000021604.jpg\n",
            "Sample/000000015660.jpg\n",
            "Sample/000000017115.jpg\n",
            "Sample/000000025228.jpg\n",
            "Sample/000000026941.jpg\n",
            "Sample/000000007795.jpg\n",
            "Sample/000000002006.jpg\n",
            "Sample/000000018150.jpg\n",
            "Sample/000000023899.jpg\n",
            "Sample/000000020333.jpg\n",
            "Sample/000000019042.jpg\n",
            "Sample/000000032570.jpg\n",
            "Sample/000000025393.jpg\n",
            "Sample/000000000776.jpg\n",
            "Sample/000000032038.jpg\n",
            "Sample/000000005503.jpg\n",
            "Sample/000000029393.jpg\n",
            "Sample/000000002923.jpg\n",
            "Sample/000000015335.jpg\n",
            "Sample/000000002473.jpg\n",
            "Sample/000000009772.jpg\n",
            "Sample/000000021167.jpg\n",
            "Sample/000000015338.jpg\n",
            "Sample/000000007574.jpg\n",
            "Sample/000000023937.jpg\n",
            "Sample/000000002149.jpg\n",
            "Sample/000000030213.jpg\n",
            "Sample/000000012576.jpg\n",
            "Sample/000000000724.jpg\n",
            "Sample/000000022589.jpg\n",
            "Sample/000000026564.jpg\n",
            "Sample/000000025560.jpg\n",
            "Sample/000000006818.jpg\n",
            "Sample/000000006771.jpg\n",
            "Sample/000000019786.jpg\n",
            "Sample/000000012639.jpg\n",
            "Sample/000000014831.jpg\n",
            "Sample/000000025386.jpg\n",
            "Sample/000000004395.jpg\n",
            "Sample/000000005037.jpg\n",
            "Sample/000000013177.jpg\n",
            "Sample/000000007784.jpg\n",
            "Sample/000000031296.jpg\n",
            "Sample/000000020992.jpg\n",
            "Sample/000000026465.jpg\n",
            "Sample/000000008690.jpg\n",
            "Sample/000000032334.jpg\n",
            "Sample/000000009448.jpg\n",
            "Sample/000000010707.jpg\n",
            "Sample/000000030828.jpg\n",
            "Sample/000000012748.jpg\n",
            "Sample/000000019742.jpg\n",
            "Sample/000000003661.jpg\n",
            "Sample/000000029397.jpg\n",
            "Sample/000000007991.jpg\n",
            "Sample/000000029187.jpg\n",
            "Sample/000000014473.jpg\n",
            "Sample/000000008277.jpg\n",
            "Sample/000000031749.jpg\n",
            "Sample/000000016249.jpg\n",
            "Sample/000000018575.jpg\n",
            "Sample/000000003156.jpg\n",
            "Sample/000000008844.jpg\n",
            "Sample/000000014439.jpg\n",
            "Sample/000000026690.jpg\n",
            "Sample/000000015956.jpg\n",
            "Sample/000000018380.jpg\n",
            "Sample/000000025603.jpg\n",
            "Sample/000000014380.jpg\n",
            "Sample/000000021879.jpg\n",
            "Sample/000000001425.jpg\n",
            "Sample/000000001993.jpg\n",
            "Sample/000000025181.jpg\n",
            "Sample/000000022969.jpg\n",
            "Sample/000000012667.jpg\n",
            "Sample/000000011615.jpg\n",
            "Sample/000000007386.jpg\n",
            "Sample/000000020571.jpg\n",
            "Sample/000000001675.jpg\n",
            "Sample/000000030494.jpg\n",
            "Sample/000000024243.jpg\n",
            "Sample/000000016502.jpg\n",
            "Sample/000000011511.jpg\n",
            "Sample/000000019221.jpg\n",
            "Sample/000000018491.jpg\n",
            "Sample/000000024021.jpg\n",
            "Sample/000000011197.jpg\n",
            "Sample/000000030504.jpg\n",
            "Sample/000000031269.jpg\n",
            "Sample/000000031217.jpg\n",
            "Sample/000000009891.jpg\n",
            "Sample/000000008532.jpg\n",
            "Sample/000000015517.jpg\n",
            "Sample/000000023230.jpg\n",
            "Sample/000000009590.jpg\n",
            "Sample/000000009400.jpg\n",
            "Sample/000000008211.jpg\n",
            "Sample/000000019109.jpg\n",
            "Sample/000000000632.jpg\n",
            "Sample/000000012280.jpg\n",
            "Sample/000000005060.jpg\n",
            "Sample/000000014007.jpg\n",
            "Sample/000000022479.jpg\n",
            "Sample/000000023272.jpg\n",
            "Sample/000000030675.jpg\n",
            "Sample/000000027972.jpg\n",
            "Sample/000000005586.jpg\n",
            "Sample/000000023023.jpg\n",
            "Sample/000000001353.jpg\n",
            "Sample/000000001296.jpg\n",
            "Sample/000000015440.jpg\n",
            "Sample/000000017436.jpg\n",
            "Sample/000000023666.jpg\n",
            "Sample/000000022371.jpg\n",
            "Sample/000000005477.jpg\n",
            "Sample/000000027982.jpg\n",
            "Sample/000000031093.jpg\n",
            "Sample/000000011122.jpg\n",
            "Sample/000000020107.jpg\n",
            "Sample/000000021503.jpg\n",
            "Sample/000000027186.jpg\n",
            "Sample/000000031620.jpg\n",
            "Sample/000000030785.jpg\n",
            "Sample/000000015497.jpg\n",
            "Sample/000000000802.jpg\n",
            "Sample/000000003934.jpg\n",
            "Sample/000000026204.jpg\n",
            "Sample/000000010092.jpg\n",
            "Sample/000000016598.jpg\n",
            "Sample/000000018833.jpg\n",
            "Sample/000000009483.jpg\n",
            "Sample/000000028993.jpg\n",
            "Sample/000000006954.jpg\n",
            "Sample/000000010764.jpg\n",
            "Sample/000000002153.jpg\n",
            "Sample/000000029675.jpg\n",
            "Sample/000000031817.jpg\n",
            "Sample/000000014226.jpg\n",
            "Sample/000000013348.jpg\n",
            "Sample/000000002532.jpg\n",
            "Sample/000000025424.jpg\n",
            "Sample/000000009769.jpg\n",
            "Sample/000000028285.jpg\n",
            "Sample/000000004765.jpg\n",
            "Sample/000000024919.jpg\n",
            "Sample/000000018193.jpg\n",
            "Sample/000000001818.jpg\n",
            "Sample/000000000885.jpg\n",
            "Sample/000000018837.jpg\n",
            "Sample/000000018770.jpg\n",
            "Sample/000000013597.jpg\n",
            "Sample/000000000785.jpg\n",
            "Sample/000000006763.jpg\n",
            "Sample/000000004795.jpg\n",
            "Sample/000000007888.jpg\n",
            "Sample/000000031248.jpg\n",
            "Sample/000000008021.jpg\n",
            "Sample/000000001584.jpg\n",
            "Sample/000000017031.jpg\n",
            "Sample/000000005001.jpg\n",
            "Sample/000000002587.jpg\n",
            "Sample/000000012120.jpg\n",
            "Sample/000000020059.jpg\n",
            "Sample/000000025394.jpg\n",
            "Sample/000000008899.jpg\n",
            "Sample/000000006723.jpg\n",
            "Sample/000000006012.jpg\n",
            "Sample/000000022755.jpg\n",
            "Sample/000000001000.jpg\n",
            "Sample/000000021903.jpg\n",
            "Sample/000000010363.jpg\n",
            "Sample/000000013004.jpg\n",
            "Sample/000000007818.jpg\n",
            "Sample/000000032285.jpg\n",
            "Sample/000000000285.jpg\n",
            "Sample/000000013729.jpg\n",
            "Sample/000000007977.jpg\n",
            "Sample/000000004134.jpg\n",
            "Sample/000000015597.jpg\n",
            "Sample/000000005992.jpg\n",
            "Sample/000000025593.jpg\n",
            "Sample/000000022705.jpg\n",
            "Sample/000000015272.jpg\n",
            "Sample/000000022892.jpg\n",
            "Sample/000000031322.jpg\n",
            "Sample/000000010977.jpg\n",
            "Sample/000000025057.jpg\n",
            "Sample/000000025139.jpg\n",
            "Sample/000000031735.jpg\n",
            "Sample/000000000872.jpg\n",
            "Sample/000000007281.jpg\n",
            "Sample/000000022192.jpg\n",
            "Sample/000000016439.jpg\n",
            "Sample/000000021465.jpg\n",
            "Sample/000000017714.jpg\n",
            "Sample/000000007278.jpg\n",
            "Sample/000000011051.jpg\n",
            "Sample/000000013291.jpg\n",
            "Sample/000000005529.jpg\n",
            "Sample/000000016010.jpg\n",
            "Sample/000000008762.jpg\n",
            "Sample/000000028449.jpg\n",
            "Sample/000000007816.jpg\n",
            "Sample/000000006040.jpg\n",
            "Sample/000000017029.jpg\n",
            "Sample/000000014038.jpg\n",
            "Sample/000000032081.jpg\n",
            "Sample/000000013201.jpg\n",
            "Sample/000000002685.jpg\n",
            "Sample/000000029640.jpg\n",
            "Sample/000000017182.jpg\n",
            "Sample/000000011149.jpg\n",
            "Sample/000000023126.jpg\n",
            "Sample/000000008629.jpg\n",
            "Sample/000000027620.jpg\n",
            "Sample/000000022935.jpg\n",
            "Sample/000000024610.jpg\n",
            "Sample/000000016228.jpg\n",
            "Sample/000000017899.jpg\n",
            "Sample/000000024027.jpg\n",
            "Sample/000000010583.jpg\n",
            "Sample/000000019432.jpg\n",
            "Sample/000000009378.jpg\n",
            "Sample/000000013923.jpg\n",
            "Sample/000000022623.jpg\n",
            "Sample/000000031050.jpg\n",
            "Sample/000000014888.jpg\n",
            "Sample/000000019402.jpg\n",
            "Sample/000000021839.jpg\n",
            "Sample/000000011813.jpg\n",
            "Sample/000000002299.jpg\n",
            "Sample/000000020247.jpg\n",
            "Sample/000000027932.jpg\n",
            "Sample/000000003255.jpg\n",
            "Sample/000000022396.jpg\n",
            "Sample/000000012670.jpg\n",
            "Sample/000000004495.jpg\n",
            "Sample/000000006894.jpg\n",
            "Sample/000000001268.jpg\n",
            "Sample/000000025986.jpg\n",
            "Sample/000000005600.jpg\n",
            "Sample/000000028809.jpg\n",
            "Sample/000000002157.jpg\n",
            "Sample/000000024567.jpg\n",
            "Sample/000000002431.jpg\n",
            "Sample/000000007108.jpg\n",
            "Sample/000000023751.jpg\n",
            "Sample/000000023781.jpg\n",
            "Sample/000000005193.jpg\n",
            "Sample/000000015079.jpg\n",
            "Sample/000000001532.jpg\n",
            "Sample/000000031118.jpg\n",
            "Sample/000000002592.jpg\n",
            "Sample/000000026926.jpg\n",
            "Sample/000000015751.jpg\n",
            "Sample/000000017627.jpg\n",
            "Sample/000000013659.jpg\n",
            "Sample/000000011760.jpg\n",
            "Sample/000000017178.jpg\n",
            "Sample/000000001761.jpg\n",
            "Sample/000000015254.jpg\n",
            "Sample/000000017379.jpg\n",
            "Sample/000000006460.jpg\n",
            "Sample/000000000139.jpg\n",
            "Sample/000000024144.jpg\n",
            "Sample/000000002261.jpg\n",
            "Sample/000000009914.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "!cat /proc/cpuinfo\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exiz0RNgLwVd",
        "outputId": "cae851fd-4f19-47f3-c411-ceb88c37cca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jun  6 06:25:43 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P0    31W /  70W |   1203MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 85\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2000.130\n",
            "cache size\t: 39424 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed\n",
            "bogomips\t: 4000.26\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 85\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "stepping\t: 3\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2000.130\n",
            "cache size\t: 39424 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed\n",
            "bogomips\t: 4000.26\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj4fvtP6ail1",
        "outputId": "7535a18a-1cf7-454d-c449-1692976a6339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.0.114-py3-none-any.whl (595 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m595.4/595.4 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.7.0.72)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.27.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.10.1)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.14.1)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.65.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (2.4.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (4.5.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.0->ultralytics) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.0->ultralytics) (0.40.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics) (1.16.0)\n",
            "Installing collected packages: ultralytics\n",
            "Successfully installed ultralytics-8.0.114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "yv8_time = 0\n",
        "c =0\n",
        "def yolov8():\n",
        "  global yv8_time\n",
        "  global c\n",
        "  model = YOLO('yolov8n.pt')\n",
        "  root = 'Sample'\n",
        "  for f in os.listdir(root):\n",
        "   \n",
        "    f_path = os.path.join(root , f)\n",
        "    results = model.predict(source = f_path)\n",
        "    for result in results:\n",
        "      yv8_time += result.speed['inference']\n",
        "      c += 1\n",
        "yolov8()\n",
        "print(\"Average Inference Time (for (640,640) : \"+str(yv8_time/c)+\"ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4RA8iWJa-w1",
        "outputId": "50c4fd1b-17bd-4d91-f767-082167654729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "image 1/1 /content/Sample/000000003845.jpg: 480x640 1 cup, 1 bowl, 2 broccolis, 2 carrots, 1 chair, 1 dining table, 16.4ms\n",
            "Speed: 2.9ms preprocess, 16.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000029984.jpg: 512x640 4 persons, 1 umbrella, 3 chairs, 12.6ms\n",
            "Speed: 1.7ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000011699.jpg: 640x480 4 persons, 3 handbags, 1 suitcase, 10.4ms\n",
            "Speed: 1.2ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000029596.jpg: 448x640 2 chairs, 1 couch, 1 dining table, 1 tv, 10.9ms\n",
            "Speed: 1.1ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000007088.jpg: 640x480 1 person, 1 truck, 1 umbrella, 11.1ms\n",
            "Speed: 1.2ms preprocess, 11.1ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000023034.jpg: 448x640 2 persons, 2 horses, 14.9ms\n",
            "Speed: 1.3ms preprocess, 14.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000003553.jpg: 448x640 2 persons, 1 skateboard, 10.1ms\n",
            "Speed: 1.2ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000012062.jpg: 448x640 4 sheeps, 11.8ms\n",
            "Speed: 1.2ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000013774.jpg: 480x640 1 person, 1 kite, 10.6ms\n",
            "Speed: 1.1ms preprocess, 10.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000007511.jpg: 480x640 3 persons, 1 backpack, 1 kite, 14.8ms\n",
            "Speed: 1.2ms preprocess, 14.8ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000016451.jpg: 640x640 1 person, 2 umbrellas, 1 handbag, 2 suitcases, 2 surfboards, 2 chairs, 14.8ms\n",
            "Speed: 4.2ms preprocess, 14.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000018737.jpg: 448x640 2 motorcycles, 10.6ms\n",
            "Speed: 1.2ms preprocess, 10.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000017905.jpg: 640x480 1 person, 1 traffic light, 10.5ms\n",
            "Speed: 1.2ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000027768.jpg: 640x640 2 persons, 2 cars, 1 bus, 13.1ms\n",
            "Speed: 2.7ms preprocess, 13.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000001490.jpg: 320x640 1 person, 1 surfboard, 12.1ms\n",
            "Speed: 1.1ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000010995.jpg: 480x640 1 bed, 14.9ms\n",
            "Speed: 1.4ms preprocess, 14.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000025096.jpg: 640x480 2 persons, 2 knifes, 1 cake, 1 dining table, 11.5ms\n",
            "Speed: 2.0ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000019924.jpg: 640x608 1 person, 1 tie, 11.4ms\n",
            "Speed: 2.6ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000016958.jpg: 448x640 3 chairs, 2 books, 4 vases, 18.4ms\n",
            "Speed: 1.4ms preprocess, 18.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000001503.jpg: 480x640 1 tv, 1 laptop, 2 mouses, 1 keyboard, 14.5ms\n",
            "Speed: 2.2ms preprocess, 14.5ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000015746.jpg: 640x448 1 fire hydrant, 16.6ms\n",
            "Speed: 1.5ms preprocess, 16.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000006213.jpg: 448x640 1 sink, 16.3ms\n",
            "Speed: 1.3ms preprocess, 16.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000017207.jpg: 480x640 3 persons, 1 car, 1 motorcycle, 1 bus, 1 truck, 17.2ms\n",
            "Speed: 1.6ms preprocess, 17.2ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000018519.jpg: 640x544 1 person, 1 sheep, 1 skateboard, 18.0ms\n",
            "Speed: 1.8ms preprocess, 18.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000006614.jpg: 512x640 1 apple, 1 orange, 14.9ms\n",
            "Speed: 3.0ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000006471.jpg: 448x640 11 persons, 1 baseball bat, 1 baseball glove, 20.7ms\n",
            "Speed: 2.6ms preprocess, 20.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000023359.jpg: 448x640 1 person, 1 snowboard, 14.7ms\n",
            "Speed: 3.6ms preprocess, 14.7ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000017959.jpg: 448x640 1 person, 9 kites, 15.2ms\n",
            "Speed: 1.5ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000003501.jpg: 640x640 1 bowl, 2 broccolis, 1 dining table, 18.6ms\n",
            "Speed: 3.7ms preprocess, 18.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000028452.jpg: 640x480 1 bottle, 1 bowl, 1 refrigerator, 19.0ms\n",
            "Speed: 1.4ms preprocess, 19.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000027696.jpg: 416x640 1 person, 1 fork, 1 pizza, 1 dining table, 14.5ms\n",
            "Speed: 1.3ms preprocess, 14.5ms inference, 4.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000015278.jpg: 480x640 6 broccolis, 14.4ms\n",
            "Speed: 1.5ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000013546.jpg: 448x640 2 persons, 1 car, 2 benchs, 2 skateboards, 21.6ms\n",
            "Speed: 1.7ms preprocess, 21.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000020553.jpg: 480x640 2 bottles, 1 vase, 1 teddy bear, 14.8ms\n",
            "Speed: 2.9ms preprocess, 14.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000021604.jpg: 640x512 1 person, 3 ties, 18.5ms\n",
            "Speed: 1.6ms preprocess, 18.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000015660.jpg: 352x640 1 person, 4 kites, 2 surfboards, 13.4ms\n",
            "Speed: 1.2ms preprocess, 13.4ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000017115.jpg: 640x448 2 zebras, 14.4ms\n",
            "Speed: 1.4ms preprocess, 14.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000025228.jpg: 448x640 1 person, 1 surfboard, 14.0ms\n",
            "Speed: 1.4ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000026941.jpg: 448x640 1 person, 7 suitcases, 13.3ms\n",
            "Speed: 1.3ms preprocess, 13.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000007795.jpg: 448x640 5 beds, 14.4ms\n",
            "Speed: 1.4ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000002006.jpg: 480x640 3 persons, 1 bus, 14.0ms\n",
            "Speed: 1.3ms preprocess, 14.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000018150.jpg: 480x640 2 persons, 1 pizza, 15.4ms\n",
            "Speed: 1.4ms preprocess, 15.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000023899.jpg: 448x640 3 persons, 2 couchs, 1 laptop, 15.5ms\n",
            "Speed: 1.4ms preprocess, 15.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000020333.jpg: 640x448 1 person, 1 bowl, 14.1ms\n",
            "Speed: 1.4ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000019042.jpg: 384x640 1 person, 1 bird, 1 sheep, 16.4ms\n",
            "Speed: 1.2ms preprocess, 16.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000032570.jpg: 448x640 1 person, 1 surfboard, 14.2ms\n",
            "Speed: 1.4ms preprocess, 14.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000025393.jpg: 480x640 2 persons, 1 car, 2 ties, 13.9ms\n",
            "Speed: 1.5ms preprocess, 13.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000000776.jpg: 640x448 4 teddy bears, 14.2ms\n",
            "Speed: 1.4ms preprocess, 14.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000032038.jpg: 640x448 1 person, 1 pizza, 14.3ms\n",
            "Speed: 3.5ms preprocess, 14.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000005503.jpg: 640x448 2 toilets, 13.4ms\n",
            "Speed: 2.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000029393.jpg: 480x640 1 dog, 1 sports ball, 13.9ms\n",
            "Speed: 2.6ms preprocess, 13.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000002923.jpg: 480x640 1 bird, 1 kite, 13.8ms\n",
            "Speed: 2.7ms preprocess, 13.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000015335.jpg: 480x640 4 persons, 1 cup, 13.4ms\n",
            "Speed: 1.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000002473.jpg: 448x640 4 persons, 1 skis, 14.2ms\n",
            "Speed: 1.4ms preprocess, 14.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000009772.jpg: 640x576 1 person, 1 bottle, 1 tv, 2 sinks, 15.3ms\n",
            "Speed: 1.8ms preprocess, 15.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000021167.jpg: 640x448 2 persons, 1 tie, 1 wine glass, 17.3ms\n",
            "Speed: 1.3ms preprocess, 17.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000015338.jpg: 448x640 2 cars, 1 bus, 14.4ms\n",
            "Speed: 1.5ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000007574.jpg: 480x640 3 bottles, 1 cup, 2 bowls, 3 sinks, 1 refrigerator, 1 vase, 19.3ms\n",
            "Speed: 1.4ms preprocess, 19.3ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000023937.jpg: 448x640 4 sheeps, 14.6ms\n",
            "Speed: 1.4ms preprocess, 14.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000002149.jpg: 448x640 1 banana, 4 apples, 14.9ms\n",
            "Speed: 1.4ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000030213.jpg: 480x640 2 bowls, 3 chairs, 1 dining table, 1 sink, 1 refrigerator, 17.9ms\n",
            "Speed: 1.5ms preprocess, 17.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000012576.jpg: 640x480 2 persons, 5 cups, 3 forks, 2 knifes, 6 pizzas, 1 dining table, 1 tv, 16.0ms\n",
            "Speed: 1.4ms preprocess, 16.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000000724.jpg: 640x480 1 truck, 2 stop signs, 15.5ms\n",
            "Speed: 2.9ms preprocess, 15.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000022589.jpg: 480x640 2 sheeps, 14.3ms\n",
            "Speed: 1.5ms preprocess, 14.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000026564.jpg: 448x640 1 tv, 1 laptop, 2 mouses, 3 keyboards, 4 books, 15.4ms\n",
            "Speed: 1.4ms preprocess, 15.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000025560.jpg: 480x640 1 person, 1 cat, 1 cup, 1 tv, 14.6ms\n",
            "Speed: 1.4ms preprocess, 14.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000006818.jpg: 640x448 1 toilet, 14.5ms\n",
            "Speed: 1.3ms preprocess, 14.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000006771.jpg: 448x640 10 persons, 14.2ms\n",
            "Speed: 1.4ms preprocess, 14.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000019786.jpg: 480x640 2 persons, 16.6ms\n",
            "Speed: 2.8ms preprocess, 16.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000012639.jpg: 640x480 17 persons, 1 baseball bat, 1 baseball glove, 14.3ms\n",
            "Speed: 5.6ms preprocess, 14.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000014831.jpg: 640x480 (no detections), 16.3ms\n",
            "Speed: 1.5ms preprocess, 16.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000025386.jpg: 640x448 2 persons, 1 chair, 14.1ms\n",
            "Speed: 1.7ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000004395.jpg: 640x448 1 person, 1 tie, 13.8ms\n",
            "Speed: 1.3ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000005037.jpg: 448x640 3 persons, 3 cars, 1 bus, 14.1ms\n",
            "Speed: 1.3ms preprocess, 14.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000013177.jpg: 448x640 1 person, 1 bicycle, 1 motorcycle, 13.0ms\n",
            "Speed: 1.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000007784.jpg: 480x640 1 kite, 14.4ms\n",
            "Speed: 3.7ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000031296.jpg: 448x640 19 persons, 4 bottles, 2 cups, 9 chairs, 1 dining table, 15.2ms\n",
            "Speed: 1.4ms preprocess, 15.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000020992.jpg: 448x640 1 person, 13.8ms\n",
            "Speed: 2.6ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000026465.jpg: 480x640 1 person, 1 laptop, 2 cell phones, 14.2ms\n",
            "Speed: 2.8ms preprocess, 14.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000008690.jpg: 480x640 3 persons, 1 dog, 13.7ms\n",
            "Speed: 1.4ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000032334.jpg: 480x640 6 persons, 4 wine glasss, 2 cups, 3 chairs, 1 dining table, 13.2ms\n",
            "Speed: 1.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000009448.jpg: 640x576 1 person, 1 umbrella, 14.3ms\n",
            "Speed: 1.7ms preprocess, 14.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000010707.jpg: 480x640 4 persons, 2 bottles, 3 cups, 3 couchs, 1 dining table, 1 laptop, 1 book, 15.3ms\n",
            "Speed: 2.4ms preprocess, 15.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000030828.jpg: 448x640 1 parking meter, 1 bench, 2 sports balls, 14.5ms\n",
            "Speed: 1.4ms preprocess, 14.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000012748.jpg: 640x480 3 persons, 1 horse, 14.3ms\n",
            "Speed: 1.4ms preprocess, 14.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000019742.jpg: 480x640 2 cups, 1 vase, 14.1ms\n",
            "Speed: 2.9ms preprocess, 14.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000003661.jpg: 384x640 1 cup, 2 bananas, 13.7ms\n",
            "Speed: 1.1ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000029397.jpg: 480x640 1 person, 15.4ms\n",
            "Speed: 1.6ms preprocess, 15.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000007991.jpg: 384x640 1 cup, 1 fork, 1 knife, 1 carrot, 1 dining table, 14.1ms\n",
            "Speed: 1.3ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000029187.jpg: 512x640 3 persons, 1 horse, 17.2ms\n",
            "Speed: 1.5ms preprocess, 17.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000014473.jpg: 448x640 6 persons, 1 train, 16.7ms\n",
            "Speed: 1.4ms preprocess, 16.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000008277.jpg: 640x640 1 fork, 2 bowls, 1 broccoli, 1 dining table, 18.1ms\n",
            "Speed: 4.0ms preprocess, 18.1ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000031749.jpg: 640x640 2 clocks, 13.8ms\n",
            "Speed: 1.8ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000016249.jpg: 480x640 7 persons, 1 bench, 1 chair, 13.8ms\n",
            "Speed: 2.9ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000018575.jpg: 480x640 2 cups, 1 orange, 1 dining table, 13.9ms\n",
            "Speed: 1.3ms preprocess, 13.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000003156.jpg: 640x448 1 person, 1 toilet, 13.9ms\n",
            "Speed: 1.4ms preprocess, 13.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000008844.jpg: 448x640 3 persons, 1 banana, 17.0ms\n",
            "Speed: 1.4ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000014439.jpg: 416x640 15 persons, 1 car, 2 backpacks, 1 kite, 1 chair, 17.0ms\n",
            "Speed: 1.4ms preprocess, 17.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000026690.jpg: 640x448 9 persons, 1 baseball glove, 1 skateboard, 14.6ms\n",
            "Speed: 1.4ms preprocess, 14.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000015956.jpg: 480x640 1 person, 1 horse, 1 clock, 14.4ms\n",
            "Speed: 1.5ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000018380.jpg: 448x640 18 persons, 2 wine glasss, 9 cups, 3 bowls, 1 dining table, 14.5ms\n",
            "Speed: 1.4ms preprocess, 14.5ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000025603.jpg: 480x640 5 persons, 2 cups, 7 chairs, 1 dining table, 1 tv, 14.3ms\n",
            "Speed: 1.4ms preprocess, 14.3ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000014380.jpg: 448x640 2 trains, 16.4ms\n",
            "Speed: 1.4ms preprocess, 16.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000021879.jpg: 448x640 4 persons, 5 surfboards, 13.7ms\n",
            "Speed: 1.4ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000001425.jpg: 512x640 1 bowl, 1 cake, 14.2ms\n",
            "Speed: 1.5ms preprocess, 14.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000001993.jpg: 448x640 1 chair, 1 bed, 2 dining tables, 15.3ms\n",
            "Speed: 1.5ms preprocess, 15.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000025181.jpg: 448x640 1 person, 1 train, 1 bench, 13.7ms\n",
            "Speed: 1.3ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000022969.jpg: 480x640 2 giraffes, 14.3ms\n",
            "Speed: 2.9ms preprocess, 14.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000012667.jpg: 480x640 1 banana, 1 remote, 19.8ms\n",
            "Speed: 1.4ms preprocess, 19.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000011615.jpg: 640x480 (no detections), 14.3ms\n",
            "Speed: 1.3ms preprocess, 14.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000007386.jpg: 448x640 1 motorcycle, 1 dog, 14.0ms\n",
            "Speed: 2.7ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000020571.jpg: 640x544 3 persons, 1 boat, 14.3ms\n",
            "Speed: 1.6ms preprocess, 14.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000001675.jpg: 480x640 1 cat, 1 keyboard, 14.5ms\n",
            "Speed: 1.4ms preprocess, 14.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000030494.jpg: 480x640 1 dog, 13.7ms\n",
            "Speed: 1.4ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000024243.jpg: 480x640 6 persons, 1 bench, 1 cup, 14.9ms\n",
            "Speed: 1.4ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000016502.jpg: 640x480 1 sheep, 16.5ms\n",
            "Speed: 2.9ms preprocess, 16.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000011511.jpg: 480x640 6 persons, 14.5ms\n",
            "Speed: 1.5ms preprocess, 14.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000019221.jpg: 480x640 1 person, 1 broccoli, 15.7ms\n",
            "Speed: 1.6ms preprocess, 15.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000018491.jpg: 448x640 9 persons, 2 baseball gloves, 16.2ms\n",
            "Speed: 1.4ms preprocess, 16.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000024021.jpg: 416x640 8 persons, 15.3ms\n",
            "Speed: 1.3ms preprocess, 15.3ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000011197.jpg: 448x640 4 persons, 1 bicycle, 5 cars, 3 traffic lights, 1 fire hydrant, 14.2ms\n",
            "Speed: 1.3ms preprocess, 14.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000030504.jpg: 640x480 1 person, 1 backpack, 1 skis, 14.0ms\n",
            "Speed: 1.4ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000031269.jpg: 480x640 3 zebras, 21.9ms\n",
            "Speed: 1.5ms preprocess, 21.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000031217.jpg: 448x640 1 person, 1 tennis racket, 14.9ms\n",
            "Speed: 1.5ms preprocess, 14.9ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000009891.jpg: 480x640 2 persons, 1 car, 1 suitcase, 19.0ms\n",
            "Speed: 1.5ms preprocess, 19.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000008532.jpg: 448x640 1 person, 1 tie, 14.6ms\n",
            "Speed: 1.4ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000015517.jpg: 480x640 5 buss, 1 train, 15.0ms\n",
            "Speed: 1.4ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000023230.jpg: 480x640 5 birds, 15.3ms\n",
            "Speed: 1.5ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000009590.jpg: 448x640 5 persons, 2 bottles, 3 cups, 3 bowls, 1 dining table, 1 clock, 17.8ms\n",
            "Speed: 1.4ms preprocess, 17.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000009400.jpg: 480x640 10 persons, 4 cups, 8 laptops, 1 mouse, 1 keyboard, 19.0ms\n",
            "Speed: 1.4ms preprocess, 19.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000008211.jpg: 480x640 2 persons, 1 motorcycle, 1 handbag, 13.2ms\n",
            "Speed: 1.4ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000019109.jpg: 448x640 9 persons, 15 motorcycles, 18.9ms\n",
            "Speed: 1.4ms preprocess, 18.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000000632.jpg: 512x640 1 bottle, 1 chair, 2 potted plants, 1 bed, 2 books, 17.7ms\n",
            "Speed: 1.8ms preprocess, 17.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000012280.jpg: 640x480 2 persons, 17.6ms\n",
            "Speed: 1.4ms preprocess, 17.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000005060.jpg: 640x480 1 person, 1 bottle, 1 chair, 23.3ms\n",
            "Speed: 1.5ms preprocess, 23.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000014007.jpg: 448x640 1 cat, 1 suitcase, 18.0ms\n",
            "Speed: 1.4ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000022479.jpg: 608x640 1 person, 1 skateboard, 17.3ms\n",
            "Speed: 2.4ms preprocess, 17.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000023272.jpg: 480x640 3 cars, 2 cats, 18.0ms\n",
            "Speed: 2.6ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000030675.jpg: 384x640 1 train, 18.0ms\n",
            "Speed: 1.2ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000027972.jpg: 448x640 1 person, 1 surfboard, 17.8ms\n",
            "Speed: 1.5ms preprocess, 17.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000005586.jpg: 480x640 5 persons, 1 tennis racket, 17.0ms\n",
            "Speed: 2.4ms preprocess, 17.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000023023.jpg: 640x640 1 suitcase, 17.5ms\n",
            "Speed: 3.6ms preprocess, 17.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000001353.jpg: 640x480 5 persons, 1 suitcase, 1 chair, 17.9ms\n",
            "Speed: 6.3ms preprocess, 17.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000001296.jpg: 640x448 3 persons, 18.5ms\n",
            "Speed: 1.3ms preprocess, 18.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000015440.jpg: 640x416 1 car, 1 stop sign, 17.9ms\n",
            "Speed: 1.3ms preprocess, 17.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000017436.jpg: 640x512 1 person, 1 bench, 1 sports ball, 17.3ms\n",
            "Speed: 1.6ms preprocess, 17.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000023666.jpg: 640x480 1 toilet, 18.0ms\n",
            "Speed: 1.4ms preprocess, 18.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000022371.jpg: 448x640 1 person, 1 tie, 1 dining table, 1 laptop, 1 cell phone, 17.6ms\n",
            "Speed: 2.5ms preprocess, 17.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000005477.jpg: 352x640 1 airplane, 17.7ms\n",
            "Speed: 1.3ms preprocess, 17.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000027982.jpg: 448x640 1 surfboard, 1 toilet, 17.5ms\n",
            "Speed: 2.6ms preprocess, 17.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000031093.jpg: 448x640 2 persons, 1 skateboard, 16.5ms\n",
            "Speed: 1.3ms preprocess, 16.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000011122.jpg: 480x640 1 stop sign, 17.4ms\n",
            "Speed: 1.5ms preprocess, 17.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000020107.jpg: 640x448 1 fire hydrant, 17.7ms\n",
            "Speed: 2.8ms preprocess, 17.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000021503.jpg: 480x640 1 cup, 1 donut, 17.5ms\n",
            "Speed: 1.4ms preprocess, 17.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000027186.jpg: 448x640 1 person, 2 couchs, 1 remote, 15.8ms\n",
            "Speed: 1.5ms preprocess, 15.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000031620.jpg: 640x480 3 persons, 1 chair, 1 couch, 17.3ms\n",
            "Speed: 1.4ms preprocess, 17.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000030785.jpg: 480x640 1 bowl, 3 broccolis, 1 cake, 1 dining table, 17.4ms\n",
            "Speed: 1.5ms preprocess, 17.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000015497.jpg: 480x640 1 cat, 1 bed, 16.8ms\n",
            "Speed: 1.4ms preprocess, 16.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000000802.jpg: 640x448 1 oven, 1 refrigerator, 20.5ms\n",
            "Speed: 1.5ms preprocess, 20.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000003934.jpg: 640x480 8 persons, 1 wine glass, 1 couch, 17.5ms\n",
            "Speed: 2.9ms preprocess, 17.5ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000026204.jpg: 448x640 3 persons, 6 cars, 1 bus, 1 truck, 17.6ms\n",
            "Speed: 1.4ms preprocess, 17.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000010092.jpg: 448x640 3 chairs, 1 bed, 1 dining table, 16.9ms\n",
            "Speed: 1.5ms preprocess, 16.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000016598.jpg: 640x480 1 person, 1 cell phone, 18.8ms\n",
            "Speed: 1.5ms preprocess, 18.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000018833.jpg: 448x640 1 cat, 17.8ms\n",
            "Speed: 1.3ms preprocess, 17.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000009483.jpg: 480x640 2 persons, 1 tie, 1 tv, 1 laptop, 1 mouse, 1 keyboard, 17.1ms\n",
            "Speed: 1.5ms preprocess, 17.1ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000028993.jpg: 640x640 3 persons, 1 fire hydrant, 1 vase, 28.4ms\n",
            "Speed: 4.1ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000006954.jpg: 480x640 6 persons, 2 frisbees, 17.8ms\n",
            "Speed: 1.5ms preprocess, 17.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000010764.jpg: 448x640 1 person, 1 baseball glove, 17.3ms\n",
            "Speed: 1.4ms preprocess, 17.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000002153.jpg: 480x640 5 persons, 2 baseball bats, 17.5ms\n",
            "Speed: 1.4ms preprocess, 17.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000029675.jpg: 640x480 4 hot dogs, 17.3ms\n",
            "Speed: 1.4ms preprocess, 17.3ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000031817.jpg: 640x352 3 persons, 17.6ms\n",
            "Speed: 1.2ms preprocess, 17.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000014226.jpg: 480x640 1 person, 1 chair, 1 laptop, 21.1ms\n",
            "Speed: 1.4ms preprocess, 21.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000013348.jpg: 448x640 1 person, 1 airplane, 17.3ms\n",
            "Speed: 1.5ms preprocess, 17.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000002532.jpg: 640x480 1 person, 1 skis, 17.6ms\n",
            "Speed: 1.4ms preprocess, 17.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000025424.jpg: 640x448 1 person, 1 sports ball, 1 tennis racket, 17.5ms\n",
            "Speed: 1.4ms preprocess, 17.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000009769.jpg: 480x640 1 person, 1 car, 1 truck, 17.7ms\n",
            "Speed: 1.4ms preprocess, 17.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000028285.jpg: 448x640 1 clock, 18.8ms\n",
            "Speed: 1.6ms preprocess, 18.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000004765.jpg: 640x640 1 person, 1 skis, 24.6ms\n",
            "Speed: 4.0ms preprocess, 24.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000024919.jpg: 448x640 3 elephants, 18.8ms\n",
            "Speed: 1.5ms preprocess, 18.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000018193.jpg: 480x640 1 person, 1 sandwich, 17.4ms\n",
            "Speed: 1.5ms preprocess, 17.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000001818.jpg: 448x640 1 zebra, 19.3ms\n",
            "Speed: 1.6ms preprocess, 19.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000000885.jpg: 448x640 4 persons, 1 tennis racket, 17.6ms\n",
            "Speed: 1.5ms preprocess, 17.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000018837.jpg: 480x640 4 persons, 1 car, 1 bus, 1 truck, 18.4ms\n",
            "Speed: 1.5ms preprocess, 18.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000018770.jpg: 448x640 1 person, 1 tie, 21.2ms\n",
            "Speed: 1.4ms preprocess, 21.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000013597.jpg: 448x640 1 cake, 18.1ms\n",
            "Speed: 1.4ms preprocess, 18.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000000785.jpg: 448x640 1 person, 1 skis, 17.2ms\n",
            "Speed: 1.4ms preprocess, 17.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000006763.jpg: 640x480 4 persons, 1 tie, 17.8ms\n",
            "Speed: 2.7ms preprocess, 17.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000004795.jpg: 480x640 1 cat, 2 laptops, 2 cell phones, 17.8ms\n",
            "Speed: 1.4ms preprocess, 17.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000007888.jpg: 640x640 1 person, 3 clocks, 17.7ms\n",
            "Speed: 2.0ms preprocess, 17.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000031248.jpg: 448x640 2 chairs, 1 couch, 2 potted plants, 3 books, 17.2ms\n",
            "Speed: 2.6ms preprocess, 17.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000008021.jpg: 480x640 3 persons, 1 tie, 19.1ms\n",
            "Speed: 1.4ms preprocess, 19.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000001584.jpg: 640x640 4 persons, 3 buss, 17.7ms\n",
            "Speed: 3.7ms preprocess, 17.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000017031.jpg: 448x640 1 person, 1 giraffe, 17.4ms\n",
            "Speed: 2.6ms preprocess, 17.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000005001.jpg: 480x640 19 persons, 1 bench, 1 cell phone, 17.6ms\n",
            "Speed: 1.4ms preprocess, 17.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000002587.jpg: 480x640 1 banana, 16.9ms\n",
            "Speed: 2.7ms preprocess, 16.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000012120.jpg: 448x640 11 persons, 1 tennis racket, 17.6ms\n",
            "Speed: 1.4ms preprocess, 17.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000020059.jpg: 448x640 2 zebras, 17.9ms\n",
            "Speed: 1.4ms preprocess, 17.9ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000025394.jpg: 640x480 3 persons, 11 bottles, 2 wine glasss, 1 cup, 1 dining table, 1 book, 17.8ms\n",
            "Speed: 1.5ms preprocess, 17.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000008899.jpg: 544x640 1 bicycle, 1 fire hydrant, 15.2ms\n",
            "Speed: 1.8ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000006723.jpg: 384x640 6 cars, 17.4ms\n",
            "Speed: 1.3ms preprocess, 17.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000006012.jpg: 544x640 3 bananas, 1 apple, 17.8ms\n",
            "Speed: 1.7ms preprocess, 17.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000022755.jpg: 480x640 2 cars, 17.0ms\n",
            "Speed: 1.4ms preprocess, 17.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000001000.jpg: 480x640 13 persons, 2 tennis rackets, 18.5ms\n",
            "Speed: 1.4ms preprocess, 18.5ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000021903.jpg: 480x640 2 persons, 1 elephant, 16.6ms\n",
            "Speed: 1.4ms preprocess, 16.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000010363.jpg: 384x640 2 cars, 1 cat, 1 bottle, 17.2ms\n",
            "Speed: 1.3ms preprocess, 17.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000013004.jpg: 640x480 1 dining table, 1 toilet, 17.5ms\n",
            "Speed: 2.8ms preprocess, 17.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000007818.jpg: 448x640 1 bottle, 1 chair, 1 potted plant, 1 book, 1 vase, 17.0ms\n",
            "Speed: 1.4ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000032285.jpg: 448x640 1 toilet, 16.8ms\n",
            "Speed: 1.3ms preprocess, 16.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000000285.jpg: 640x608 1 bear, 19.6ms\n",
            "Speed: 1.9ms preprocess, 19.6ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000013729.jpg: 480x640 4 persons, 1 backpack, 2 bottles, 1 couch, 1 dining table, 1 tv, 3 remotes, 18.2ms\n",
            "Speed: 1.6ms preprocess, 18.2ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000007977.jpg: 640x448 1 person, 1 skateboard, 19.8ms\n",
            "Speed: 1.6ms preprocess, 19.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000004134.jpg: 448x640 15 persons, 1 tie, 2 wine glasss, 1 cup, 4 dining tables, 1 laptop, 17.2ms\n",
            "Speed: 1.6ms preprocess, 17.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000015597.jpg: 640x448 1 person, 1 skateboard, 17.7ms\n",
            "Speed: 1.3ms preprocess, 17.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000005992.jpg: 448x640 1 car, 5 sheeps, 17.5ms\n",
            "Speed: 1.3ms preprocess, 17.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000025593.jpg: 480x640 1 traffic light, 18.0ms\n",
            "Speed: 1.7ms preprocess, 18.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000022705.jpg: 640x512 1 person, 1 bottle, 1 bowl, 2 refrigerators, 17.5ms\n",
            "Speed: 1.6ms preprocess, 17.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000015272.jpg: 640x448 (no detections), 17.5ms\n",
            "Speed: 1.5ms preprocess, 17.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000022892.jpg: 448x640 1 cat, 2 dogs, 17.3ms\n",
            "Speed: 2.7ms preprocess, 17.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000031322.jpg: 448x640 5 boats, 9 birds, 15.4ms\n",
            "Speed: 1.5ms preprocess, 15.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000010977.jpg: 480x640 2 bottles, 1 toilet, 1 sink, 18.3ms\n",
            "Speed: 3.3ms preprocess, 18.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000025057.jpg: 448x640 2 persons, 1 frisbee, 17.9ms\n",
            "Speed: 1.4ms preprocess, 17.9ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000025139.jpg: 448x640 1 zebra, 18.2ms\n",
            "Speed: 2.5ms preprocess, 18.2ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000031735.jpg: 480x640 1 couch, 3 potted plants, 1 dining table, 2 vases, 17.2ms\n",
            "Speed: 1.5ms preprocess, 17.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000000872.jpg: 640x640 2 persons, 17.6ms\n",
            "Speed: 2.0ms preprocess, 17.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000007281.jpg: 384x640 8 persons, 2 horses, 18.1ms\n",
            "Speed: 1.2ms preprocess, 18.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000022192.jpg: 448x640 1 dog, 1 bed, 1 book, 17.8ms\n",
            "Speed: 1.4ms preprocess, 17.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000016439.jpg: 480x640 1 wine glass, 1 laptop, 1 book, 19.4ms\n",
            "Speed: 1.4ms preprocess, 19.4ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000021465.jpg: 384x640 1 person, 1 chair, 2 dining tables, 1 vase, 18.0ms\n",
            "Speed: 2.4ms preprocess, 18.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000017714.jpg: 480x640 2 cups, 2 knifes, 1 pizza, 1 dining table, 17.7ms\n",
            "Speed: 1.5ms preprocess, 17.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000007278.jpg: 512x640 1 person, 16.9ms\n",
            "Speed: 1.7ms preprocess, 16.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000011051.jpg: 544x640 2 persons, 1 tie, 18.0ms\n",
            "Speed: 1.6ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000013291.jpg: 448x640 5 persons, 4 frisbees, 18.0ms\n",
            "Speed: 2.6ms preprocess, 18.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000005529.jpg: 640x448 1 person, 1 skis, 20.7ms\n",
            "Speed: 1.4ms preprocess, 20.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000016010.jpg: 480x640 4 sheeps, 2 zebras, 17.9ms\n",
            "Speed: 1.6ms preprocess, 17.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000008762.jpg: 448x640 6 traffic lights, 17.9ms\n",
            "Speed: 1.4ms preprocess, 17.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000028449.jpg: 448x640 5 elephants, 15.8ms\n",
            "Speed: 1.4ms preprocess, 15.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000007816.jpg: 448x640 9 persons, 1 motorcycle, 16.7ms\n",
            "Speed: 1.4ms preprocess, 16.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000006040.jpg: 352x640 3 persons, 1 car, 1 train, 16.9ms\n",
            "Speed: 1.2ms preprocess, 16.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000017029.jpg: 640x640 3 cars, 1 dog, 1 bear, 1 frisbee, 21.4ms\n",
            "Speed: 1.9ms preprocess, 21.4ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000014038.jpg: 448x640 1 sports ball, 1 chair, 1 potted plant, 1 bed, 1 tv, 1 refrigerator, 19.3ms\n",
            "Speed: 1.5ms preprocess, 19.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000032081.jpg: 640x480 1 person, 1 tennis racket, 17.0ms\n",
            "Speed: 2.7ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000013201.jpg: 640x448 1 person, 1 skateboard, 17.2ms\n",
            "Speed: 1.3ms preprocess, 17.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000002685.jpg: 576x640 7 persons, 17.6ms\n",
            "Speed: 1.8ms preprocess, 17.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000029640.jpg: 448x640 1 spoon, 1 bowl, 12 broccolis, 3 carrots, 18.7ms\n",
            "Speed: 1.6ms preprocess, 18.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000017182.jpg: 448x640 1 chair, 2 books, 20.9ms\n",
            "Speed: 1.3ms preprocess, 20.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000011149.jpg: 480x640 2 persons, 2 bicycles, 1 motorcycle, 10.8ms\n",
            "Speed: 2.0ms preprocess, 10.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000023126.jpg: 448x640 1 person, 1 horse, 10.9ms\n",
            "Speed: 1.2ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000008629.jpg: 640x640 1 fork, 6 pizzas, 1 dining table, 11.5ms\n",
            "Speed: 1.6ms preprocess, 11.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000027620.jpg: 480x640 1 cup, 2 chairs, 1 laptop, 1 mouse, 2 keyboards, 10.5ms\n",
            "Speed: 1.2ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000022935.jpg: 480x640 2 persons, 1 sports ball, 11.7ms\n",
            "Speed: 1.4ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000024610.jpg: 480x640 1 backpack, 1 suitcase, 1 bottle, 1 chair, 1 laptop, 1 book, 10.0ms\n",
            "Speed: 1.1ms preprocess, 10.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000016228.jpg: 448x640 12 persons, 1 bench, 1 horse, 1 umbrella, 11.3ms\n",
            "Speed: 1.2ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000017899.jpg: 640x480 1 person, 5 cups, 1 bowl, 1 cake, 3 chairs, 1 couch, 1 dining table, 13.6ms\n",
            "Speed: 1.2ms preprocess, 13.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000024027.jpg: 480x640 2 persons, 1 kite, 15.2ms\n",
            "Speed: 2.0ms preprocess, 15.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000010583.jpg: 640x640 1 sandwich, 1 dining table, 10.9ms\n",
            "Speed: 2.8ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000019432.jpg: 480x640 1 person, 1 sports ball, 1 tennis racket, 11 chairs, 10.6ms\n",
            "Speed: 1.2ms preprocess, 10.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000009378.jpg: 448x640 4 persons, 1 frisbee, 10.6ms\n",
            "Speed: 2.0ms preprocess, 10.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000013923.jpg: 448x640 4 chairs, 1 couch, 3 potted plants, 1 dining table, 1 tv, 1 vase, 9.7ms\n",
            "Speed: 1.1ms preprocess, 9.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000022623.jpg: 480x640 1 dining table, 10.2ms\n",
            "Speed: 1.1ms preprocess, 10.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000031050.jpg: 640x448 4 vases, 10.4ms\n",
            "Speed: 1.1ms preprocess, 10.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000014888.jpg: 480x640 1 cow, 1 elephant, 10.2ms\n",
            "Speed: 1.2ms preprocess, 10.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000019402.jpg: 480x640 3 persons, 9.5ms\n",
            "Speed: 1.1ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000021839.jpg: 640x480 7 persons, 1 car, 1 traffic light, 10.2ms\n",
            "Speed: 1.1ms preprocess, 10.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000011813.jpg: 640x448 (no detections), 10.1ms\n",
            "Speed: 1.7ms preprocess, 10.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000002299.jpg: 416x640 19 persons, 2 ties, 10.4ms\n",
            "Speed: 1.6ms preprocess, 10.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000020247.jpg: 480x640 2 bears, 10.2ms\n",
            "Speed: 1.2ms preprocess, 10.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000027932.jpg: 640x608 1 motorcycle, 10.5ms\n",
            "Speed: 2.2ms preprocess, 10.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000003255.jpg: 384x640 7 persons, 10.5ms\n",
            "Speed: 1.0ms preprocess, 10.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000022396.jpg: 512x640 1 airplane, 10.8ms\n",
            "Speed: 1.2ms preprocess, 10.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000012670.jpg: 448x640 17 persons, 10.5ms\n",
            "Speed: 1.1ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000004495.jpg: 480x640 1 chair, 1 couch, 1 tv, 10.2ms\n",
            "Speed: 1.8ms preprocess, 10.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000006894.jpg: 480x640 1 person, 2 elephants, 13.4ms\n",
            "Speed: 1.2ms preprocess, 13.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000001268.jpg: 448x640 4 persons, 2 boats, 1 bird, 1 backpack, 2 handbags, 13.5ms\n",
            "Speed: 1.1ms preprocess, 13.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000025986.jpg: 480x640 1 cup, 1 spoon, 4 bowls, 1 dining table, 10.3ms\n",
            "Speed: 1.2ms preprocess, 10.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000005600.jpg: 384x640 1 spoon, 2 bowls, 3 oranges, 1 dining table, 10.1ms\n",
            "Speed: 1.0ms preprocess, 10.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000028809.jpg: 640x640 1 bowl, 5 bananas, 10.4ms\n",
            "Speed: 2.6ms preprocess, 10.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000002157.jpg: 448x640 5 wine glasss, 1 knife, 2 bowls, 1 carrot, 1 dining table, 10.7ms\n",
            "Speed: 1.1ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000024567.jpg: 640x480 1 person, 10 hot dogs, 1 chair, 10.3ms\n",
            "Speed: 1.1ms preprocess, 10.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000002431.jpg: 640x480 1 person, 1 wine glass, 2 cups, 1 knife, 1 spoon, 1 bowl, 1 chair, 1 dining table, 10.0ms\n",
            "Speed: 1.3ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000007108.jpg: 448x640 5 elephants, 10.4ms\n",
            "Speed: 1.1ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000023751.jpg: 640x448 1 person, 10.6ms\n",
            "Speed: 1.1ms preprocess, 10.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000023781.jpg: 448x640 1 bowl, 2 broccolis, 4 carrots, 10.3ms\n",
            "Speed: 1.0ms preprocess, 10.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000005193.jpg: 448x640 6 persons, 3 surfboards, 1 bottle, 1 dining table, 1 cell phone, 16.7ms\n",
            "Speed: 1.4ms preprocess, 16.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000015079.jpg: 448x640 1 knife, 1 sandwich, 2 cakes, 1 dining table, 9.9ms\n",
            "Speed: 1.1ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000001532.jpg: 480x640 4 cars, 1 truck, 13.0ms\n",
            "Speed: 1.1ms preprocess, 13.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000031118.jpg: 448x640 5 persons, 1 car, 1 traffic light, 2 clocks, 13.2ms\n",
            "Speed: 1.5ms preprocess, 13.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000002592.jpg: 384x640 1 cup, 10.7ms\n",
            "Speed: 1.0ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000026926.jpg: 640x448 2 cars, 1 fire hydrant, 10.4ms\n",
            "Speed: 1.1ms preprocess, 10.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000015751.jpg: 448x640 1 person, 10.4ms\n",
            "Speed: 1.1ms preprocess, 10.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000017627.jpg: 480x640 3 persons, 10 cars, 10.5ms\n",
            "Speed: 1.1ms preprocess, 10.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000013659.jpg: 480x640 6 persons, 1 cup, 6 chairs, 1 dining table, 2 tvs, 4 laptops, 10.1ms\n",
            "Speed: 1.2ms preprocess, 10.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000011760.jpg: 448x640 3 zebras, 10.5ms\n",
            "Speed: 1.0ms preprocess, 10.5ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000017178.jpg: 448x640 1 car, 2 horses, 1 cow, 9.9ms\n",
            "Speed: 1.0ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000001761.jpg: 640x448 2 airplanes, 10.7ms\n",
            "Speed: 1.0ms preprocess, 10.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000015254.jpg: 448x640 1 fork, 4 bowls, 3 carrots, 1 dining table, 10.6ms\n",
            "Speed: 1.1ms preprocess, 10.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000017379.jpg: 640x480 1 tv, 2 sinks, 10.6ms\n",
            "Speed: 1.3ms preprocess, 10.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000006460.jpg: 448x640 1 person, 1 motorcycle, 1 skateboard, 1 surfboard, 11.7ms\n",
            "Speed: 1.1ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000000139.jpg: 448x640 1 person, 5 chairs, 1 potted plant, 2 dining tables, 1 tv, 1 refrigerator, 1 clock, 1 vase, 10.2ms\n",
            "Speed: 1.6ms preprocess, 10.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000024144.jpg: 480x640 1 pizza, 10.4ms\n",
            "Speed: 1.2ms preprocess, 10.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000002261.jpg: 448x640 1 person, 10.4ms\n",
            "Speed: 1.1ms preprocess, 10.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/Sample/000000009914.jpg: 480x640 1 cup, 1 knife, 2 bowls, 1 sandwich, 10.8ms\n",
            "Speed: 1.2ms preprocess, 10.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Inference Time (for (640,640) : 15.338517502692847ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms as T\n",
        "import torchvision\n",
        "model = torchvision.models.detection.ssd300_vgg16(pretrained = True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "def ssd():\n",
        "  infr = 0\n",
        "  c = 0\n",
        "  for d in os.listdir('Sample'):\n",
        "      src = os.path.join('Sample' , d)\n",
        "      model.eval()\n",
        "      #print(src)\n",
        "      \n",
        "      ig = Image.open(src)\n",
        "      width, height = ig.size\n",
        "      ig = ig.resize((640,640))\n",
        "      resized_width, resized_height = ig.size\n",
        "      scale_x = width / resized_width\n",
        "      scale_y = height / resized_height\n",
        "\n",
        "      \n",
        "      transform = T.ToTensor()\n",
        "      img = transform(ig)\n",
        "      img = img.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          start = time.time()\n",
        "          pred = model([img])\n",
        "          infr += time.time()-start\n",
        "          c+=1\n",
        "\n",
        "      bboxes, scores, labels = pred[0][\"boxes\"], pred[0][\"scores\"], pred[0][\"labels\"]\n",
        "      print(\"Average Inference Time: \"+str(infr/c)+\"s\")\n",
        "ssd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqpvhpcHg-jY",
        "outputId": "5346892f-64d5-4aa8-a476-689b1fb3e8b0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Inference Time: 0.05208611488342285s\n",
            "Average Inference Time: 0.05126690864562988s\n",
            "Average Inference Time: 0.05134876569112142s\n",
            "Average Inference Time: 0.05123108625411987s\n",
            "Average Inference Time: 0.049093103408813475s\n",
            "Average Inference Time: 0.047435482343037925s\n",
            "Average Inference Time: 0.046717166900634766s\n",
            "Average Inference Time: 0.04543152451515198s\n",
            "Average Inference Time: 0.04470915264553494s\n",
            "Average Inference Time: 0.043990802764892575s\n",
            "Average Inference Time: 0.04332232475280762s\n",
            "Average Inference Time: 0.04266538222630819s\n",
            "Average Inference Time: 0.043060559492844805s\n",
            "Average Inference Time: 0.04244547230856759s\n",
            "Average Inference Time: 0.0417853037516276s\n",
            "Average Inference Time: 0.041195809841156006s\n",
            "Average Inference Time: 0.040702988119686354s\n",
            "Average Inference Time: 0.04025230142805311s\n",
            "Average Inference Time: 0.0399367934779117s\n",
            "Average Inference Time: 0.03958535194396973s\n",
            "Average Inference Time: 0.039204347701299755s\n",
            "Average Inference Time: 0.03889334201812744s\n",
            "Average Inference Time: 0.038652990175330124s\n",
            "Average Inference Time: 0.03844305872917175s\n",
            "Average Inference Time: 0.03824709892272949s\n",
            "Average Inference Time: 0.03800844229184664s\n",
            "Average Inference Time: 0.03776171472337511s\n",
            "Average Inference Time: 0.03752445323126657s\n",
            "Average Inference Time: 0.0372994850421774s\n",
            "Average Inference Time: 0.03708904584248861s\n",
            "Average Inference Time: 0.03696751594543457s\n",
            "Average Inference Time: 0.03679473698139191s\n",
            "Average Inference Time: 0.03664097641453599s\n",
            "Average Inference Time: 0.03651358800775865s\n",
            "Average Inference Time: 0.036355236598423546s\n",
            "Average Inference Time: 0.03623333904478285s\n",
            "Average Inference Time: 0.03608294435449549s\n",
            "Average Inference Time: 0.035938231568587456s\n",
            "Average Inference Time: 0.03583997335189428s\n",
            "Average Inference Time: 0.03574405312538147s\n",
            "Average Inference Time: 0.035657370962747716s\n",
            "Average Inference Time: 0.03570709342048282s\n",
            "Average Inference Time: 0.0356088072754616s\n",
            "Average Inference Time: 0.035531038587743584s\n",
            "Average Inference Time: 0.035469547907511396s\n",
            "Average Inference Time: 0.035356568253558616s\n",
            "Average Inference Time: 0.0352978858541935s\n",
            "Average Inference Time: 0.03521782656510671s\n",
            "Average Inference Time: 0.03514703439206493s\n",
            "Average Inference Time: 0.035059142112731936s\n",
            "Average Inference Time: 0.03499898256040087s\n",
            "Average Inference Time: 0.03491114653073824s\n",
            "Average Inference Time: 0.03486641847862387s\n",
            "Average Inference Time: 0.034798224767049156s\n",
            "Average Inference Time: 0.03477259549227628s\n",
            "Average Inference Time: 0.03471753426960537s\n",
            "Average Inference Time: 0.034677810836256595s\n",
            "Average Inference Time: 0.03464925289154053s\n",
            "Average Inference Time: 0.034597186718956896s\n",
            "Average Inference Time: 0.034549486637115476s\n",
            "Average Inference Time: 0.0347035322032991s\n",
            "Average Inference Time: 0.03466470395365069s\n",
            "Average Inference Time: 0.034606365930466426s\n",
            "Average Inference Time: 0.03455083817243576s\n",
            "Average Inference Time: 0.034500503540039064s\n",
            "Average Inference Time: 0.03451722679716168s\n",
            "Average Inference Time: 0.034459299115992305s\n",
            "Average Inference Time: 0.03442025184631348s\n",
            "Average Inference Time: 0.03438267846038376s\n",
            "Average Inference Time: 0.03437648160117013s\n",
            "Average Inference Time: 0.034337208304606695s\n",
            "Average Inference Time: 0.034332719114091664s\n",
            "Average Inference Time: 0.03430007581841456s\n",
            "Average Inference Time: 0.034260411520262025s\n",
            "Average Inference Time: 0.03423868179321289s\n",
            "Average Inference Time: 0.034184007268202934s\n",
            "Average Inference Time: 0.034169850411353175s\n",
            "Average Inference Time: 0.03413572678199181s\n",
            "Average Inference Time: 0.034103650081006784s\n",
            "Average Inference Time: 0.034082213044166566s\n",
            "Average Inference Time: 0.034058956452357916s\n",
            "Average Inference Time: 0.034029931556887744s\n",
            "Average Inference Time: 0.0340146616280797s\n",
            "Average Inference Time: 0.03398761295136951s\n",
            "Average Inference Time: 0.0339629986706902s\n",
            "Average Inference Time: 0.03394393865452256s\n",
            "Average Inference Time: 0.03398586141652074s\n",
            "Average Inference Time: 0.03395299478010698s\n",
            "Average Inference Time: 0.033935804045602175s\n",
            "Average Inference Time: 0.03393500910864936s\n",
            "Average Inference Time: 0.03391955711029388s\n",
            "Average Inference Time: 0.03389876821766729s\n",
            "Average Inference Time: 0.03387517057439332s\n",
            "Average Inference Time: 0.03385667597993891s\n",
            "Average Inference Time: 0.03383068536457263s\n",
            "Average Inference Time: 0.03380706409613291s\n",
            "Average Inference Time: 0.033805355583269574s\n",
            "Average Inference Time: 0.03378967606291479s\n",
            "Average Inference Time: 0.033791845495050606s\n",
            "Average Inference Time: 0.03377945184707642s\n",
            "Average Inference Time: 0.033859840714105284s\n",
            "Average Inference Time: 0.033965078054689894s\n",
            "Average Inference Time: 0.03395422916967892s\n",
            "Average Inference Time: 0.033925569974459134s\n",
            "Average Inference Time: 0.03390715235755557s\n",
            "Average Inference Time: 0.033892654023080504s\n",
            "Average Inference Time: 0.03387548990338762s\n",
            "Average Inference Time: 0.03385086633540966s\n",
            "Average Inference Time: 0.0338819267552927s\n",
            "Average Inference Time: 0.033852852474559436s\n",
            "Average Inference Time: 0.033842460529224294s\n",
            "Average Inference Time: 0.03388695844582149s\n",
            "Average Inference Time: 0.03387572280073588s\n",
            "Average Inference Time: 0.033849762197126425s\n",
            "Average Inference Time: 0.03383653682211171s\n",
            "Average Inference Time: 0.03381223719695519s\n",
            "Average Inference Time: 0.033795943626990683s\n",
            "Average Inference Time: 0.033774185988862636s\n",
            "Average Inference Time: 0.03377171925136021s\n",
            "Average Inference Time: 0.033750381072362265s\n",
            "Average Inference Time: 0.03374195492957249s\n",
            "Average Inference Time: 0.033751663614491945s\n",
            "Average Inference Time: 0.0337219664720985s\n",
            "Average Inference Time: 0.033698956812581705s\n",
            "Average Inference Time: 0.03369281959533692s\n",
            "Average Inference Time: 0.03367365352691166s\n",
            "Average Inference Time: 0.033676186884482075s\n",
            "Average Inference Time: 0.033652570098638535s\n",
            "Average Inference Time: 0.03363834240639857s\n",
            "Average Inference Time: 0.03362684066479023s\n",
            "Average Inference Time: 0.0336250112256931s\n",
            "Average Inference Time: 0.033619178063941726s\n",
            "Average Inference Time: 0.033606217319804024s\n",
            "Average Inference Time: 0.03361490591248469s\n",
            "Average Inference Time: 0.033618153466118704s\n",
            "Average Inference Time: 0.03363715725786546s\n",
            "Average Inference Time: 0.033628716085949084s\n",
            "Average Inference Time: 0.033617071483446205s\n",
            "Average Inference Time: 0.033602841466450865s\n",
            "Average Inference Time: 0.033584788867405485s\n",
            "Average Inference Time: 0.0335787221895042s\n",
            "Average Inference Time: 0.03356034990767358s\n",
            "Average Inference Time: 0.033547834916548294s\n",
            "Average Inference Time: 0.033540498879220754s\n",
            "Average Inference Time: 0.033533323222193226s\n",
            "Average Inference Time: 0.033525657980409386s\n",
            "Average Inference Time: 0.033510140010288784s\n",
            "Average Inference Time: 0.03349358326679951s\n",
            "Average Inference Time: 0.03347191874612898s\n",
            "Average Inference Time: 0.03346327940622965s\n",
            "Average Inference Time: 0.033450717168138515s\n",
            "Average Inference Time: 0.033458940292659556s\n",
            "Average Inference Time: 0.03344710979586333s\n",
            "Average Inference Time: 0.03344546819662119s\n",
            "Average Inference Time: 0.03343480940788023s\n",
            "Average Inference Time: 0.03343352904686561s\n",
            "Average Inference Time: 0.03341841849551839s\n",
            "Average Inference Time: 0.03339943402930151s\n",
            "Average Inference Time: 0.03342560702149973s\n",
            "Average Inference Time: 0.03341900855302811s\n",
            "Average Inference Time: 0.033405797081704464s\n",
            "Average Inference Time: 0.033390928197790076s\n",
            "Average Inference Time: 0.03340489147630937s\n",
            "Average Inference Time: 0.033396866263412844s\n",
            "Average Inference Time: 0.03339288740447073s\n",
            "Average Inference Time: 0.033392419298011136s\n",
            "Average Inference Time: 0.033382789817398895s\n",
            "Average Inference Time: 0.03337079570406959s\n",
            "Average Inference Time: 0.03336494914173375s\n",
            "Average Inference Time: 0.03336133395924288s\n",
            "Average Inference Time: 0.033356684690330464s\n",
            "Average Inference Time: 0.0333512966022935s\n",
            "Average Inference Time: 0.03334171923598802s\n",
            "Average Inference Time: 0.033331872402936565s\n",
            "Average Inference Time: 0.0333161735534668s\n",
            "Average Inference Time: 0.03333349525928497s\n",
            "Average Inference Time: 0.033327481167464604s\n",
            "Average Inference Time: 0.03330930431237381s\n",
            "Average Inference Time: 0.0333245186832364s\n",
            "Average Inference Time: 0.033320384555392796s\n",
            "Average Inference Time: 0.03330846912953076s\n",
            "Average Inference Time: 0.033295352380354325s\n",
            "Average Inference Time: 0.03329382698392608s\n",
            "Average Inference Time: 0.03327783164770707s\n",
            "Average Inference Time: 0.03327448303635056s\n",
            "Average Inference Time: 0.03326263350825156s\n",
            "Average Inference Time: 0.033260411757198884s\n",
            "Average Inference Time: 0.03324797559291758s\n",
            "Average Inference Time: 0.033229620999129364s\n",
            "Average Inference Time: 0.033220958709716794s\n",
            "Average Inference Time: 0.03321060210622418s\n",
            "Average Inference Time: 0.03321403016646703s\n",
            "Average Inference Time: 0.03322333993071719s\n",
            "Average Inference Time: 0.033222635996710395s\n",
            "Average Inference Time: 0.03321393086360051s\n",
            "Average Inference Time: 0.033206110097924055s\n",
            "Average Inference Time: 0.033198998059113013s\n",
            "Average Inference Time: 0.03319724280424793s\n",
            "Average Inference Time: 0.03319499001431106s\n",
            "Average Inference Time: 0.03318977355957031s\n",
            "Average Inference Time: 0.033181288942175716s\n",
            "Average Inference Time: 0.03317725776445748s\n",
            "Average Inference Time: 0.033179212673544296s\n",
            "Average Inference Time: 0.03317109977497774s\n",
            "Average Inference Time: 0.03317206545573909s\n",
            "Average Inference Time: 0.033162270934836376s\n",
            "Average Inference Time: 0.03315655390421549s\n",
            "Average Inference Time: 0.03314542082639841s\n",
            "Average Inference Time: 0.03313090926722476s\n",
            "Average Inference Time: 0.03313045274643671s\n",
            "Average Inference Time: 0.03311995533405322s\n",
            "Average Inference Time: 0.0331572136788998s\n",
            "Average Inference Time: 0.03320295486092008s\n",
            "Average Inference Time: 0.03322460606833485s\n",
            "Average Inference Time: 0.03324447232623433s\n",
            "Average Inference Time: 0.033267443930661236s\n",
            "Average Inference Time: 0.03328941384768156s\n",
            "Average Inference Time: 0.03331231086625965s\n",
            "Average Inference Time: 0.033342644504216166s\n",
            "Average Inference Time: 0.033380621129816226s\n",
            "Average Inference Time: 0.03340354008911962s\n",
            "Average Inference Time: 0.03341999354663196s\n",
            "Average Inference Time: 0.033444749934790914s\n",
            "Average Inference Time: 0.033477761915751865s\n",
            "Average Inference Time: 0.033505487442016604s\n",
            "Average Inference Time: 0.0335253888526849s\n",
            "Average Inference Time: 0.03354638053457117s\n",
            "Average Inference Time: 0.033575382149010374s\n",
            "Average Inference Time: 0.03360066351411645s\n",
            "Average Inference Time: 0.03361990451812744s\n",
            "Average Inference Time: 0.03365085548136657s\n",
            "Average Inference Time: 0.03367204604477718s\n",
            "Average Inference Time: 0.03369412401715062s\n",
            "Average Inference Time: 0.03371509119995639s\n",
            "Average Inference Time: 0.033744690266061333s\n",
            "Average Inference Time: 0.03377716116986032s\n",
            "Average Inference Time: 0.033803577664532236s\n",
            "Average Inference Time: 0.03382211673159559s\n",
            "Average Inference Time: 0.033841933166631595s\n",
            "Average Inference Time: 0.03393287559350332s\n",
            "Average Inference Time: 0.03396205783384964s\n",
            "Average Inference Time: 0.03398350644702754s\n",
            "Average Inference Time: 0.03402909722347809s\n",
            "Average Inference Time: 0.034050084528375844s\n",
            "Average Inference Time: 0.03407479597597706s\n",
            "Average Inference Time: 0.034108879120369265s\n",
            "Average Inference Time: 0.0341447548345033s\n",
            "Average Inference Time: 0.03418131701407894s\n",
            "Average Inference Time: 0.03423368308438834s\n",
            "Average Inference Time: 0.03427932834625244s\n",
            "Average Inference Time: 0.03432956349802207s\n",
            "Average Inference Time: 0.03438385327657064s\n",
            "Average Inference Time: 0.0345396637445382s\n",
            "Average Inference Time: 0.034583034477834625s\n",
            "Average Inference Time: 0.0346336561090806s\n",
            "Average Inference Time: 0.03468372020870447s\n",
            "Average Inference Time: 0.0347281550618925s\n",
            "Average Inference Time: 0.03477546965429025s\n",
            "Average Inference Time: 0.03482521915067577s\n",
            "Average Inference Time: 0.03488355875015259s\n",
            "Average Inference Time: 0.034937286742345586s\n",
            "Average Inference Time: 0.034986303962824s\n",
            "Average Inference Time: 0.035037852058845784s\n",
            "Average Inference Time: 0.035100814971056854s\n",
            "Average Inference Time: 0.03514677533563578s\n",
            "Average Inference Time: 0.03519421412532491s\n",
            "Average Inference Time: 0.0352625846862793s\n",
            "Average Inference Time: 0.035306404775647976s\n",
            "Average Inference Time: 0.035347790523089444s\n",
            "Average Inference Time: 0.035394823992693863s\n",
            "Average Inference Time: 0.035439808870153675s\n",
            "Average Inference Time: 0.03548067808151245s\n",
            "Average Inference Time: 0.03552903011168316s\n",
            "Average Inference Time: 0.03557610685807945s\n",
            "Average Inference Time: 0.03562184940684925s\n",
            "Average Inference Time: 0.03566762761793275s\n",
            "Average Inference Time: 0.03576240642836808s\n",
            "Average Inference Time: 0.03580765329676566s\n",
            "Average Inference Time: 0.03585621789364832s\n",
            "Average Inference Time: 0.03589428322655814s\n",
            "Average Inference Time: 0.035938743170470105s\n",
            "Average Inference Time: 0.03600264187400223s\n",
            "Average Inference Time: 0.03604831459665467s\n",
            "Average Inference Time: 0.036088378496573s\n",
            "Average Inference Time: 0.03612872424878572s\n",
            "Average Inference Time: 0.03617311941160189s\n",
            "Average Inference Time: 0.03620817021625798s\n",
            "Average Inference Time: 0.0362461076842414s\n",
            "Average Inference Time: 0.03627497102150042s\n",
            "Average Inference Time: 0.036317332859697016s\n",
            "Average Inference Time: 0.03640697502188666s\n",
            "Average Inference Time: 0.03644077663552271s\n",
            "Average Inference Time: 0.036492266345756454s\n",
            "Average Inference Time: 0.03653170948936826s\n",
            "Average Inference Time: 0.0365705239570747s\n",
            "Average Inference Time: 0.0366078486313691s\n",
            "Average Inference Time: 0.036647425757514104s\n",
            "Average Inference Time: 0.03670057194344949s\n",
            "Average Inference Time: 0.036733368966093034s\n",
            "Average Inference Time: 0.03677985350290934s\n",
            "Average Inference Time: 0.036776285234875854s\n"
          ]
        }
      ]
    }
  ]
}